<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coords="1,115.97,107.03,363.29,12.90">Swin Transformer V2: Scaling Up Capacity and Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,77.90,147.01,32.21,10.37"><forename type="first">Ze</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,125.05,147.01,37.52,10.37"><forename type="first">Han</forename><surname>Hu</surname></persName>
							<email>hanhu@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,185.76,147.01,54.13,10.37"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
							<email>t-yutonglin@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,254.84,147.01,64.88,10.37"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,334.66,147.01,105.59,10.37"><forename type="first">Zhenda</forename><forename type="middle">Xie</forename><surname>Yixuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,443.24,147.01,47.19,10.37"><forename type="first">Wei</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,493.41,147.01,23.91,10.37;1,148.17,160.96,18.59,10.37"><forename type="first">Ning</forename><surname>Yue</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,169.75,160.96,64.75,10.37"><forename type="first">Cao</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,237.48,160.96,56.12,10.37"><forename type="first">Zhang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,296.59,160.96,64.09,10.37"><forename type="first">Dong</forename><surname>Furu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,363.67,160.96,95.34,10.37"><forename type="first">Wei</forename><forename type="middle">Baining</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coords="1,115.97,107.03,363.29,12.90">Swin Transformer V2: Scaling Up Capacity and Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CF8F1FA2342BD588837818C089DCD351</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-02-18T17:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large-scale NLP models have been shown to significantly improve the performance on language tasks with no signs of saturation. They also demonstrate amazing few-shot capabilities like that of human beings. This paper aims to explore large-scale models in computer vision. We tackle three major issues in training and application of large vision models, including training instability, resolution gaps between pre-training and fine-tuning, and hunger on labelled data. Three main techniques are proposed: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) A log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) A self-supervised pretraining method, SimMIM, to reduce the needs of vast labeled images. Through these techniques, this paper successfully trained a 3 billion-parameter Swin Transformer V2 model, which is the largest dense vision model to date, and makes it capable of training with images of up to 1,536×1,536 resolution. It set new performance records on 4 representative vision tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification. Also note our training is much more efficient than that in Google's billion-level visual models, which consumes 40 times less labelled data and 40 times less training time. Code is available at https://github.com/ microsoft/Swin-Transformer.</p><p>* Equal. † Project lead. Ze, Yutong, Zhuliang, Zhenda, Yixuan, Jia are long-term interns at MSRA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text>
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scaling up language models has been incredibly successful. It significantly improves a model's performance on language tasks <ref type="bibr" coords="1,101.03,674.11,15.77,8.64" target="#b18">[19,</ref><ref type="bibr" coords="1,118.83,674.11,12.45,8.64" target="#b23">24,</ref><ref type="bibr" coords="1,133.32,674.11,12.45,8.64" target="#b48">49,</ref><ref type="bibr" coords="1,147.81,674.11,12.45,8.64" target="#b49">50,</ref><ref type="bibr" coords="1,162.29,674.11,12.45,8.64" target="#b51">52,</ref><ref type="bibr" coords="1,176.79,674.11,13.28,8.64" target="#b52">53]</ref> and the model demon-Figure <ref type="figure" coords="1,335.17,420.80,3.36,7.77">1</ref>. To better scale up model capacity and window resolution, several adaptions are made on the original Swin Transformer architecture (V1): 1) A res-post-norm to replace the previous prenorm configuration; 2) A scaled cosine attention to replace the original dot product attention; 3) A log-spaced continuous relative position bias approach to replace the previous parameterized approach. Adaptions 1) and 2) make it easier for the model to scale up capacity. Adaption 3) makes the model to be transferred more effectively across window resolutions. The adapted architecture is named Swin Transformer V2.</p><p>strates amazing few-shot capabilities similar to that of human beings <ref type="bibr" coords="1,357.07,560.35,10.58,8.64" target="#b6">[7]</ref>. Since the BERT large model with 340 million parameters <ref type="bibr" coords="1,375.48,572.30,15.27,8.64" target="#b18">[19]</ref>, language models are quickly scaled up by more than 1,000 times in a few years, reaching 530 billion dense parameters <ref type="bibr" coords="1,408.84,596.21,16.60,8.64" target="#b49">[50]</ref> and 1.6 trillion sparse parameters <ref type="bibr" coords="1,330.88,608.17,15.27,8.64" target="#b23">[24]</ref>. These large language models are also found to possess increasingly strong few-shot capabilities akin to human intelligence for a broad range of language tasks <ref type="bibr" coords="1,519.96,632.08,10.58,8.64" target="#b6">[7]</ref>.</p><p>On the other hand, the scaling up of vision models has been lagging behind. While it has long been recognized that larger vision models usually perform better on vision tasks <ref type="bibr" coords="1,331.03,680.60,16.20,8.64" target="#b28">[29,</ref><ref type="bibr" coords="1,347.23,680.60,12.15,8.64" target="#b59">60]</ref>, the absolute model size was just able to reach about 1-2 billion parameters very recently <ref type="bibr" coords="1,475.01,692.56,16.50,8.64" target="#b16">[17,</ref><ref type="bibr" coords="1,491.50,692.56,12.37,8.64" target="#b26">27,</ref><ref type="bibr" coords="1,503.88,692.56,12.37,8.64" target="#b38">39,</ref><ref type="bibr" coords="1,516.25,692.56,12.37,8.64" target="#b55">56,</ref><ref type="bibr" coords="1,528.62,692.56,12.37,8.64" target="#b79">80]</ref>. More importantly, unlike large language models, the exist-arXiv:2111.09883v2 [cs.CV] 11 Apr 2022 ing large vision models are applied to the image classification task only <ref type="bibr" coords="2,106.85,87.43,15.77,8.64" target="#b16">[17,</ref><ref type="bibr" coords="2,123.91,87.43,12.45,8.64" target="#b55">56,</ref><ref type="bibr" coords="2,137.66,87.43,11.83,8.64" target="#b79">80]</ref>.</p><p>To successfully train large and general vision model, we need to address a few key issues. Firstly, our experiments with large vision models reveal an instability issue in training. We find that the discrepancy of activation amplitudes across layers becomes significantly greater in large models. A closer look at the original architecture reveals that this is caused by the output of the residual unit directly added back to the main branch. The result is that the activation values are accumulated layer by layer, and the amplitudes at deeper layers are thus significantly larger than those at early layers. To address this issue, we propose a new normalization configuration, called res-post-norm, which moves the LN layer from the beginning of each residual unit to the backend, as shown in Figure <ref type="figure" coords="2,149.91,257.26,3.74,8.64">1</ref>. We find this new configuration produces much milder activation values across the network layers. We also propose a scaled cosine attention to replace the previous dot product attention. The scaled cosine attention makes the computation irrelevant to amplitudes of block inputs, and the attention values are less likely to fall into extremes. In our experiments, the proposed two techniques not only make the training process more stable but also improve the accuracy especially for larger models.</p><p>Secondly, many downstream vision tasks such as object detection and semantic segmentation require high resolution input images or large attention windows. The window size variations between low-resolution pre-training and high-resolution fine-tuning can be quite large. The current common practice is to perform a bi-cubic interpolation of the position bias maps <ref type="bibr" coords="2,145.98,439.04,15.77,8.64" target="#b21">[22,</ref><ref type="bibr" coords="2,163.80,439.04,11.83,8.64" target="#b45">46]</ref>. This simple fix is somewhat ad-hoc and the result is usually sub-optimal. We introduce a log-spaced continuous position bias (Log-CPB), which generates bias values for arbitrary coordinate ranges by applying a small meta network on the log-spaced coordinate inputs. Since the meta network takes any coordinates, a pre-trained model will be able to freely transfer across window sizes by sharing weights of the meta network. A critical design of our approach is to transform the coordinates into the log-space so that the extrapolation ratio can be low even when the target window size is significantly larger than that of pre-training. The scaling up of model capacity and resolution also leads to prohibitively high GPU memory consumption with existing vision models. To resolve the memory issue, we incorporate several important techniques including zero-optimizer <ref type="bibr" coords="2,238.07,618.37,15.27,8.64" target="#b53">[54]</ref>, activation check pointing <ref type="bibr" coords="2,130.26,630.33,16.60,8.64" target="#b11">[12]</ref> and a novel implementation of sequential self-attention computation. With these techniques, the GPU memory consumption of large models and resolutions is significantly reduced with only marginal effect on the training speed.</p><p>With the above techniques, we successfully trained a 3 billion Swin Transformer model and effectively transferred it to various vision tasks with image resolution as large as 1,536×1,536, using Nvidia A100-40G GPUs. In our model pre-training, we also employ self-supervised pre-training to reduce the dependency on super-huge labeled data. With 40× less labelled data than that in previous practice (JFT-3B), the 3 billion model achieves the state-of-the-art accuracy on a broad range of vision benchmarks. Specifically, it obtains 84.0% top-1 accuracy on the ImageNet-V2 image classification validation set <ref type="bibr" coords="2,419.16,171.12,15.27,8.64" target="#b54">[55]</ref>, 63.1 / 54.4 box / mask AP on the COCO test-dev set of object detection, 59.9 mIoU on ADE20K semantic segmentation, and 86.8% top-1 accuracy on Kinetics-400 video action classification, which are +NA%, +4.4/+3.3, +6.3 and +1.9 higher than the best numbers in the original Swin Transformers <ref type="bibr" coords="2,464.15,230.89,15.77,8.64" target="#b45">[46,</ref><ref type="bibr" coords="2,480.94,230.89,11.83,8.64" target="#b46">47]</ref>, and surpass previous best records by +0.8% ( <ref type="bibr" coords="2,447.89,242.85,14.94,8.64" target="#b79">[80]</ref>), +1.8/+1.4 ( [74]), +1.5 <ref type="bibr" coords="2,329.42,254.81,17.01,8.64">( [4]</ref>) and +1.4% ( <ref type="bibr" coords="2,404.20,254.81,14.94,8.64" target="#b56">[57]</ref>).</p><p>By scaling up both capacity and resolution of vision models with strong performance on general vision tasks, just like a good language model's performance on general NLP tasks, we aim to stimulate more research in this direction so that we can eventually close the capacity gap between vision and language models and facilitate the joint modeling of the two domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Language networks and scaling up Transformer has served the standard network since the pioneer work of <ref type="bibr" coords="2,526.03,397.92,15.27,8.64" target="#b64">[65]</ref>. The exploration of scaling this architecture has since begun, and the progress has been accelerated by the invention of effective self-supervised learning approaches, such as masked or auto-regressive language modeling <ref type="bibr" coords="2,459.76,445.74,16.15,8.64" target="#b18">[19,</ref><ref type="bibr" coords="2,475.91,445.74,12.12,8.64" target="#b51">52]</ref>, and has been further encouraged by the discovery of a scaling law <ref type="bibr" coords="2,526.03,457.70,15.27,8.64" target="#b35">[36]</ref>. Since then, the capacity of language models has increased dramatically by more than 1,000 times in a few years, from BERT-340M to the Megatron-Turing-530B <ref type="bibr" coords="2,489.24,493.56,10.79,8.64" target="#b6">[7,</ref><ref type="bibr" coords="2,502.34,493.56,12.45,8.64" target="#b48">49,</ref><ref type="bibr" coords="2,517.08,493.56,12.45,8.64" target="#b49">50,</ref><ref type="bibr" coords="2,531.84,493.56,13.28,8.64" target="#b52">53]</ref> and sparse Switch-Transformer-1.6T <ref type="bibr" coords="2,460.10,505.52,15.27,8.64" target="#b23">[24]</ref>. With increased capacity, the accuracy of various language benchmarks has been significantly improved. The zero-shot or few-shot performance is also significantly improved <ref type="bibr" coords="2,466.09,541.38,10.58,8.64" target="#b6">[7]</ref>, which is a foundation of human generic intelligence.</p><p>Vision networks and scaling up CNNs have long been the standard computer vision networks <ref type="bibr" coords="2,478.00,596.92,15.77,8.64" target="#b39">[40,</ref><ref type="bibr" coords="2,496.71,596.92,11.83,8.64" target="#b40">41]</ref>. Since AlexNet <ref type="bibr" coords="2,344.92,608.87,15.27,8.64" target="#b39">[40]</ref>, architectures have become deeper and larger, which has greatly advanced various visual tasks and largely fueled the wave of deep learning in computer vision, such as VGG <ref type="bibr" coords="2,333.37,644.74,15.27,8.64" target="#b59">[60]</ref>, GoogleNet <ref type="bibr" coords="2,402.48,644.74,16.60,8.64" target="#b61">[62]</ref> and ResNet citehe2015resnet. In the past two years, the CNN architectures have been further scaled up to about 1 billion parameters <ref type="bibr" coords="2,488.98,668.65,15.77,8.64" target="#b26">[27,</ref><ref type="bibr" coords="2,506.24,668.65,11.83,8.64" target="#b38">39]</ref>, however, absolute performance may not be so encouraging, perhaps due to inductive biases in the CNN architecture limiting modeling power.</p><p>Last year, Transformers started taking over one representative visual benchmark after another, including ImageNet-1K image-level classification benchmarks <ref type="bibr" coords="3,232.81,99.39,15.27,8.64" target="#b21">[22]</ref>, COCO region-level object detection benchmark <ref type="bibr" coords="3,223.74,111.34,15.27,8.64" target="#b45">[46]</ref>, ADE20K pixel-level semantic segmentation benchmark <ref type="bibr" coords="3,250.85,123.30,15.77,8.64" target="#b45">[46,</ref><ref type="bibr" coords="3,270.59,123.30,11.83,8.64" target="#b82">83]</ref>, Kinetics-400 video action classification benchmark <ref type="bibr" coords="3,255.82,135.25,10.58,8.64" target="#b1">[2]</ref>, etc. Since these works, numerous vision Transformer variants have been proposed to improve the accuracy at relatively small scale <ref type="bibr" coords="3,97.12,171.12,15.77,8.64" target="#b13">[14,</ref><ref type="bibr" coords="3,114.35,171.12,12.45,8.64" target="#b20">21,</ref><ref type="bibr" coords="3,128.27,171.12,12.45,8.64" target="#b33">34,</ref><ref type="bibr" coords="3,142.19,171.12,12.45,8.64" target="#b41">42,</ref><ref type="bibr" coords="3,156.11,171.12,12.45,8.64" target="#b62">63,</ref><ref type="bibr" coords="3,170.03,171.12,12.45,8.64" target="#b67">68,</ref><ref type="bibr" coords="3,183.95,171.12,12.45,8.64" target="#b70">71,</ref><ref type="bibr" coords="3,197.87,171.12,12.45,8.64" target="#b74">75,</ref><ref type="bibr" coords="3,211.80,171.12,12.45,8.64" target="#b76">77,</ref><ref type="bibr" coords="3,225.71,171.12,12.45,8.64" target="#b77">78,</ref><ref type="bibr" coords="3,239.63,171.12,11.83,8.64" target="#b81">82]</ref>. Only a few works have attempted to scale up the vision Transformers <ref type="bibr" coords="3,64.12,195.03,15.77,8.64" target="#b16">[17,</ref><ref type="bibr" coords="3,81.01,195.03,12.45,8.64" target="#b55">56,</ref><ref type="bibr" coords="3,94.59,195.03,11.83,8.64" target="#b79">80]</ref>. However, they rely on a huge image dataset with classification labels, i.e., JFT-3B, and are only applied to image classification problems.</p><p>Transferring across window / kernel resolution For CNNs, previous works typically fixed kernel size during pre-training and fine-tuning. Global vision Transformers, such as ViT <ref type="bibr" coords="3,107.24,285.16,15.27,8.64" target="#b21">[22]</ref>, compute attention globally, with the equivalent attention window size linearly proportional to the increased input image resolution. For local vision Transformer architectures, such as Swin Transformer <ref type="bibr" coords="3,250.85,321.02,15.27,8.64" target="#b45">[46]</ref>, the window size can be either fixed or changed during finetuning. Allowing variable window sizes is more convenient in use, so as to be divisible by the probably variable entire feature map and to tune receptive fields for better accuracy. To handle the variable window sizes between pretraining and fine-tuning, bi-cubic interpolation was the previous common practice <ref type="bibr" coords="3,146.90,404.71,15.77,8.64" target="#b21">[22,</ref><ref type="bibr" coords="3,164.26,404.71,11.83,8.64" target="#b45">46]</ref>. In this paper, we propose a log-spaced continuous position bias approach (Log-CPB) that more smoothly transfers pre-trained model weights at low resolution to deal-with higher resolution windows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study on bias terms</head><p>In NLP, the relative position bias method proved beneficial <ref type="bibr" coords="3,158.26,482.88,15.27,8.64" target="#b52">[53]</ref>, compared to the absolute position embedding used in the original Transformer <ref type="bibr" coords="3,267.28,494.84,15.27,8.64" target="#b64">[65]</ref>. In computer vision, the relative positional bias method is more commonly used <ref type="bibr" coords="3,137.50,518.75,16.36,8.64" target="#b30">[31,</ref><ref type="bibr" coords="3,153.86,518.75,12.27,8.64" target="#b45">46,</ref><ref type="bibr" coords="3,166.13,518.75,12.27,8.64" target="#b74">75]</ref>, probably because the spatial relationships of visual signals play a more important role in visual modeling. A common practice is to directly learn the bias values as model weights. There are also a few works particularly study how to set and learn the bias terms <ref type="bibr" coords="3,74.74,578.52,15.77,8.64" target="#b37">[38,</ref><ref type="bibr" coords="3,91.81,578.52,11.83,8.64" target="#b68">69]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Continuous convolution and variants</head><p>Our Log-CPB approach is also related to earlier works on continuous convolution and variants <ref type="bibr" coords="3,134.48,632.78,15.77,8.64" target="#b29">[30,</ref><ref type="bibr" coords="3,152.45,632.78,12.45,8.64" target="#b44">45,</ref><ref type="bibr" coords="3,167.11,632.78,12.45,8.64" target="#b57">58,</ref><ref type="bibr" coords="3,181.77,632.78,11.83,8.64" target="#b66">67]</ref>, which utilize a meta network to handle irregular data points. Our Log-CPB approach is inspired by these efforts while solving a different problem of transferring relative position biases in vision Transformers across arbitrary window sizes. We also propose log-spaced coordinates to alleviate the difficulty of extrapolation when transferring between large size changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Swin Transformer V2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">A Brief Review of Swin Transformer</head><p>Swin Transformer is a general-purpose computer vision backbone that has achieved strong performance in various granular recognition tasks such as region-level object detection, pixel-level semantic segmentation, and imagelevel image classification. The main idea of Swin Transformer is to introduce several important visual priors into the vanilla Transformer encoder, including hierarchy, locality, and translation invariance, which combines the strength of both: the basic Transformer unit has strong modeling capabilities, and the visual priors make it friendly to a variety of visual tasks.</p><p>Normalization configuration It is widely known that normalization technologies <ref type="bibr" coords="3,424.75,269.86,10.79,8.64" target="#b2">[3,</ref><ref type="bibr" coords="3,438.57,269.86,12.45,8.64" target="#b34">35,</ref><ref type="bibr" coords="3,454.05,269.86,12.45,8.64" target="#b63">64,</ref><ref type="bibr" coords="3,469.53,269.86,13.28,8.64" target="#b69">70]</ref> are crucial in stably training deeper architectures. The original Swin Transformer inherits the common practice in the language Transformers <ref type="bibr" coords="3,366.62,305.72,16.60,8.64" target="#b51">[52]</ref> and vanilla ViT <ref type="bibr" coords="3,457.65,305.72,16.60,8.64" target="#b21">[22]</ref> to utilize a prenormalization configuration without extensive study, as shown in the figure <ref type="figure" coords="3,387.68,329.63,3.74,8.64">1</ref>. In the following subsections, we will examine this default normalization configuration <ref type="foot" coords="3,502.86,339.92,3.49,6.05" target="#foot_0">1</ref> .</p><p>Relative position bias is a key component in the original Swin Transformer which introduces an additional parametric bias term to encode the geometric relationship in selfattention calculation:</p><formula xml:id="formula_0" coords="3,318.27,413.23,226.85,17.93">Attention(Q, K, V ) = SoftMax(QK T / √ d + B)V,<label>(1)</label></formula><p>where B ∈ R M 2 ×M 2 is the relative position bias term for each head; Q, K, V ∈ R M 2 ×d are the query, key and value matrices; d is the query/key dimension, and M 2 is the number of patches in a window. The relative position bias encodes relative spatial configurations of visual elements and is shown critical in a variety of visual tasks, especially for dense recognition tasks such as object detection.</p><p>In Swin Transformer, the relative positions along each axis are within the range of [-M + 1, M -1] and the relative position bias is parameterized as a bias matrix B ∈ R (2M -1)×(2M -1) , and the elements in B are taken from B. When transferring across different window sizes, the learnt relative position bias matrix in pre-training is used to initialize the bias matrix of a different size in fine-tuning by bi-cubic interpolation.</p><p>Issues in scaling up model capacity and window resolution We observe two issues when we scale up the capacity and window resolution of the Swin Transformer. 47.8</p><formula xml:id="formula_1" coords="4,516.19,153.89,19.77,6.91">(+3.3)</formula><p>Table <ref type="table" coords="4,72.97,175.16,3.36,7.77">1</ref>. Comparison of different position bias computation approaches using Swin-T. * indicates the top-1 accuracy on ImageNet-1k trained from scratch. The models in * column will be used for testing on the ImageNet-1K image classification task using larger image/window resolutions, marked by †. For these results, we report both the results w.o./with fine-tuning. These models are also used for fine-tuning on COCO object detection and ADE20K semantic segmentation tasks. • An instability issue when scaling up model capacity.</p><p>As shown in Figure <ref type="figure" coords="4,153.37,437.94,3.74,8.64" target="#fig_0">2</ref>, when we scale up the original Swin Transformer model from small size to large size, the activation values at deeper layers increase dramatically. The discrepancy between layers with the highest and the lowest amplitudes has reached an extreme value of 10 4 . When we scale it up further to a huge size (658 million parameters), it cannot complete the training, as shown in Figure <ref type="figure" coords="4,183.80,521.63,3.74,8.64" target="#fig_1">3</ref>.</p><p>• Degraded performance when transferring models across window resolutions. As shown in the first row of Table <ref type="table" coords="4,127.55,564.77,3.74,8.64">1</ref>, the accuracy decreases significantly when we directly test the accuracy of a pre-trained ImageNet-1K model (256 × 256 images with 8 × 8 window size) at larger image resolutions and window sizes through the bi-cubic interpolation approach. It may be worth re-examining the relative position bias approach in the original Swin Transformer.</p><p>In the following subsections, we present techniques to address these issues, including residual post normalization and scaled cosine attention to address the instability issue, and a log-spaced continuous position bias approach to address the issue in transferring across window resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Scaling Up Model Capacity</head><p>As mentioned in Section 3.1, the original Swin Transformer (and most vision Transformers) adopts a layer norm layer at the beginning of each block, inherited from vanilla ViT. When we scale up the model capacity, a significant increase in activation values is observed at deeper layers. In fact, in a pre-normalization configuration, the output activation values of each residual block are merged directly back to the main branch, and the amplitude of the main branch grows larger and larger at deeper layers. Large amplitude discrepancy in different layers causes training instability.</p><p>Post normalization To ease this problem, we propose to use a residual post normalization approach instead, as shown in Figure <ref type="figure" coords="4,380.23,407.52,3.74,8.64">1</ref>. In this approach, the output of each residual block is normalized before merging back into the main branch, and the amplitude of the main branch does not accumulate when the layer goes deeper. As shown in Figure <ref type="figure" coords="4,324.48,455.34,3.74,8.64" target="#fig_0">2</ref>, the activation amplitudes by this approach are much milder than in the original pre-normalization configuration.</p><p>In our largest model training, we introduce an additional layer normalization layer on the main branch every 6 Transformer blocks, to further stabilize training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scaled cosine attention</head><p>In the original self-attention computation, the similarity terms of the pixel pairs are computed as a dot product of the query and key vectors. We find that when this approach is used in large visual models, the learnt attention maps of some blocks and heads are frequently dominated by a few pixel pairs, especially in the res-post-norm configuration. To ease this issue, we propose a scaled cosine attention approach that computes the attention logit of a pixel pair i and j by a scaled cosine function:</p><formula xml:id="formula_2" coords="4,353.90,646.62,191.21,9.68">Sim(q i , k j ) = cos(q i , k j )/τ + B ij ,<label>(2)</label></formula><p>where B ij is the relative position bias between pixel i and j; τ is a learnable scalar, non-shared across heads and layers. τ is set larger than 0.01. The cosine function is naturally normalized, and thus can have milder attention values. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Scaling Up Window Resolution</head><p>In this subsection, we introduce a log-spaced continuous position bias approach, so that the relative position bias can be smoothly transferred across window resolutions.</p><p>Continuous relative position bias Instead of directly optimizing the parameterized biases, the continuous position bias approach adopts a small meta network on the relative coordinates:</p><formula xml:id="formula_3" coords="5,113.21,366.99,173.15,8.96">B(∆x, ∆y) = G(∆x, ∆y),<label>(3)</label></formula><p>where G is a small network, e.g., a 2-layer MLP with a ReLU activation in between by default. The meta network G generates bias values for arbitrary relative coordinates, and thus can be naturally transferred to fine-tuning tasks with arbitrarily varying window sizes. In inference, the bias values at each relative position can be pre-computed and stored as model parameters, such that the inference is the same as the original parameterized bias approach.</p><p>Log-spaced coordinates When transferring across largely varying window sizes, a large portion of the relative coordinate range needs to be extrapolated. To ease this issue, we propose using log-spaced coordinates instead of the original linear-spaced ones:</p><formula xml:id="formula_4" coords="5,105.94,579.54,180.42,26.52">∆x = sign(x) • log(1 + |∆x|), ∆y = sign(y) • log(1 + |∆y|),<label>(4)</label></formula><p>where ∆x, ∆y and ∆x, ∆y are the linear-scaled and logspaced coordinates, respectively.</p><p>By using the log-spaced coordinates, when we transfer the relative position biases across window resolutions, the required extrapolation ratio will be much smaller than that of using the original linear-spaced coordinates. For an example of transferring from a pre-trained 8 × 8 window size to a fine-tuned 16 × 16 window size, using the original raw coordinates, the input coordinate range will be from [-7, 7]× <ref type="bibr" coords="5,343.85,87.11,29.50,8.74">[-7, 7]</ref> to [-15, 15]× <ref type="bibr" coords="5,430.22,87.11,38.14,8.74">[-15, 15]</ref>. The extrapolation ratio is 8  7 = 1.14× of the original range. Using log-spaced coordinates, the input range will be from [-2.079, 2.079] × [-2.079, 2.079] to [-2.773, 2.773] × [-2.773, 2.773]. The extrapolation ratio is 0.33× of the original range, which is an about 4 times smaller extrapolation ratio than that using the original linear-spaced coordinates.</p><p>Table <ref type="table" coords="5,346.64,171.12,4.98,8.64">1</ref> compares the transferring performance of different position bias computation approaches. It can be seen that the log-spaced CPB (continuous position bias) approach performs best, particularly when transferred to larger window sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Self-Supervised Pre-training</head><p>Larger models are more data hungry. To address the data hungry problem, previous large vision models typically utilize huge labelled data such as JFT-3B <ref type="bibr" coords="5,482.42,280.59,15.77,8.64" target="#b16">[17,</ref><ref type="bibr" coords="5,500.21,280.59,12.45,8.64" target="#b55">56,</ref><ref type="bibr" coords="5,514.69,280.59,11.83,8.64" target="#b79">80]</ref>. In this work, we exploit a self-supervised pre-training method, SimMIM <ref type="bibr" coords="5,349.74,304.50,15.27,8.64" target="#b71">[72]</ref>, to alleviate the demands on labelled data. By this approach, we successfully trained a powerful Swin Transformer model of 3 billion parameters which achieves state-of-the-art (SOTA) on 4 representative visual benchmarks, by using only 70 million labelled images (1/40 of that in JFT-3B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation to Save GPU Memory</head><p>Another issue lies in the unaffordable GPU memory consumption with a regular implementation when both the capacity and resolution are large. To facility the memory issue, we adopt the following implementations:</p><p>• Zero-Redundancy Optimizer (ZeRO) <ref type="bibr" coords="5,476.18,457.56,15.27,8.64" target="#b53">[54]</ref>. In a general data-parallel implementation of optimizers, the model parameters and optimization states are broadcasted to every GPU. This implementation is very unfriendly on GPU memory consumption, for example, a model of 3 billion parameters will consume 48G GPU memory when an AdamW optimizer and fp32 weights/states are used. With a ZeRO optimizer, the model parameters and the corresponding optimization states will be split and distributed to multiple GPUs, which significantly reduces memory consumption. We adopt the DeepSpeed framework and use the ZeRO stage-1 option in our experiments. This optimization has little effect on training speed.</p><p>• Activation check-pointing <ref type="bibr" coords="5,436.35,632.78,15.27,8.64" target="#b11">[12]</ref>. Feature maps in the Transformer layers also consume a lot of GPU memory, which can create bottlenecks when image and window resolutions are high. The activation checkpointing technology can significantly reduce the memory consumption, while the training speed is up to 30% slower.</p><p>• Sequential self-attention computation. To train large models on very large resolutions, for example, an image of 1,536×1,536 resolution with a window size of 32×32, regular A100 GPUs (40GB memory) are still unaffordable, even with the above two optimization technologies. We found that in this case, the selfattention module constitutes a bottleneck. To alleviate this problem, we implement self-attention computation sequentially, instead of using the previous batch computation approach. This optimization is applied to the layers in the first two stages and has little impact on the overall training speed.</p><p>With these implementations, we managed to train a 3B model using the Nvidia A100-40G GPUs for COCO object detection with an input image resolution of 1,536×1,536, and Kinetics-400 action classification with an input resolution of 320 × 320 × 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Model configurations</head><p>We maintain the stage, block, and channel settings of the original Swin Transformer for 4 configurations of Swin Transformer V2:</p><formula xml:id="formula_5" coords="6,61.57,355.62,224.79,28.78">• SwinV2-T: C = 96, #. block = {2, 2, 6, 2} • SwinV2-S/B/L: C=96/128/192, #.block={2, 2, 18, 2}</formula><p>with C the number of channels in the first stage.</p><p>We further scale up Swin Transformer V2 to its huge size and giant size, with 658 million parameters and 3 billion parameters, respectively:</p><formula xml:id="formula_6" coords="6,61.57,450.78,189.39,28.79">• SwinV2-H: C = 352, #. block = {2, 2, 18, 2} • SwinV2-G: C = 512, #. block = {2, 2, 42, 4}</formula><p>For SwinV2-H and SwinV2-G, we add an additional layer normalization layer on the main branch every 6 layers. To save experimental time, we only employ SwinV2-G for large-scale experiments. SwinV2-H is employed for another parallel study about self-supervised learning <ref type="bibr" coords="6,252.09,538.48,15.27,8.64" target="#b71">[72]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Tasks and Datasets</head><p>We conduct experiments on ImageNet-1K image classification (V1 and V2) <ref type="bibr" coords="6,155.98,611.11,15.77,8.64" target="#b17">[18,</ref><ref type="bibr" coords="6,175.38,611.11,11.83,8.64" target="#b54">55]</ref>, COCO object detection <ref type="bibr" coords="6,69.23,623.06,15.27,8.64" target="#b43">[44]</ref>, and ADE20K semantic segmentation <ref type="bibr" coords="6,247.11,623.06,15.27,8.64" target="#b84">[85]</ref>. For the 3B model experiments, we also report the accuracy on Kinetics-400 video action recognition <ref type="bibr" coords="6,203.41,646.97,15.27,8.64" target="#b36">[37]</ref>.</p><p>• Image classification. ImageNet-1K V1 and V2 val are used <ref type="bibr" coords="6,90.27,680.60,16.85,8.64" target="#b17">[18,</ref><ref type="bibr" coords="6,107.12,680.60,12.63,8.64" target="#b54">55]</ref> for evaluation. ImageNet-22K <ref type="bibr" coords="6,243.45,680.60,16.60,8.64" target="#b17">[18]</ref> which has 14M images and 22K categories is optionally employed for pre-training. For the pre-training our largest model SwinV2-G, a privately collected ImageNet-22K-ext dataset with 70 million images is used. For this dataset, a duplicate removal process <ref type="bibr" coords="6,497.28,99.39,16.60,8.64" target="#b50">[51]</ref> is conducted to exclude overlapping images with ImageNet-1K V1 and V2 validation sets.</p><p>• Object detection. COCO <ref type="bibr" coords="6,434.79,144.98,16.60,8.64" target="#b43">[44]</ref> is used for evaluation. For our largest model experiments, we employ an additional detection pre-training phase using Object 365 v2 dataset <ref type="bibr" coords="6,359.87,180.85,15.27,8.64" target="#b58">[59]</ref>, in-between the image classification pretraining phase and the COCO fine-tuning phase.</p><p>• Semantic segmentation. ADE20K <ref type="bibr" coords="6,466.23,214.49,16.60,8.64" target="#b84">[85]</ref> is used.</p><p>• Video action classification. Kinetics-400 (K400) <ref type="bibr" coords="6,528.51,236.18,16.60,8.64" target="#b36">[37]</ref> is used in evaluation.</p><p>The pre-training and fine-tuning settings will be detailed in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Scaling Up Experiments</head><p>We first present the results on various representative visual benchmarks by scaling up models to 3 billion parameters and to high image/window resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Settings for SwinV2-G experiments</head><p>We adopt a smaller 192 × 192 image resolution in pre-training to save on training costs. We take a 2-step pre-training approach. First, the model is pre-trained using a self-supervised method <ref type="bibr" coords="6,516.27,412.11,16.60,8.64" target="#b71">[72]</ref> on the ImageNet-22K-ext dataset by 20 epochs. Second, the model is further pre-trained by 30 epochs using the image classification task on this dataset. Detailed pre-training and fine-tuning setups are described in the appendix.</p><p>In the following paragraphs, we report the accuracy of SwinV2-G on representative vision benchmarks. Note that since our main goal is to explore how to feasibly scale up model capacity and window resolution, and whether the vision tasks can benefit from significantly larger capacity, we did not particularly align complexities or pre-training data in comparisons. <ref type="table" coords="6,516.92,573.01,4.98,8.64" target="#tab_1">2</ref> compares the SwinV2-G model with previously largest/best vision models on ImageNet-1K V1 and V2 classification. SwinV2-G is the largest dense vision model to present. It achieves a top-1 accuracy of 84.0% on the ImageNet V2 benchmark, which is +0.7% higher than previous best one (83.3%). Our accuracy on ImageNet-1K V1 is marginally lower (90.17% vs 90.88%). The performance difference might come from different degrees of dataset overtuning <ref type="bibr" coords="6,338.12,680.60,15.27,8.64" target="#b54">[55]</ref>. Also note we employ much less training iterations and lower image resolutions than those in previous efforts, while performing very well. We also compare the SwinV2-B and SwinV2-L to the original SwinV1-B and SwinV1-L, respectively, where a +0.8% and +0.4% gains are observed. The shrunken gains by SwinV2-L than that of SwinV2-B may imply that if exceeding this size, more labeled data, stronger regularization, or advanced self-supervised learning methods are required.   <ref type="bibr" coords="7,522.71,415.29,14.94,8.64" target="#b73">[74]</ref>). This suggests that scaling up vision model is beneficial for the dense vision recognition task of object detection. Our approach can use a different window size at test to additionally benefit, probably attributed to the effective Log-spaced CPB approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet-1K image classification results Table</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COCO object detection results</head><p>ADE20K semantic segmentation results Table <ref type="table" coords="7,516.04,500.12,4.98,8.64" target="#tab_3">4</ref> compares the SwinV2-G model with previous best results on the ADE20K semantic segmentation benchmark. It achieves 59.9 mIoU on ADE20K val set, +1.5 higher than the previous best number (58.4 by <ref type="bibr" coords="7,423.45,547.95,10.45,8.64" target="#b3">[4]</ref>). This suggests scaling up vision model is beneficial for pixel-level vision recognition tasks. Using a larger window size at test time can additionally bring +0.2 gains, probably attributed to the effective Log-spaced CPB approach.</p><p>Kinetics-400 video action classification results Table <ref type="table" coords="7,540.13,620.83,4.98,8.64" target="#tab_4">5</ref> compares the SwinV2-G model with previous best results on the Kinetics-400 action classification benchmark. It achieves 86.8% top-1 accuracy, +1.4% higher than previous best number <ref type="bibr" coords="7,359.77,668.65,15.27,8.64" target="#b56">[57]</ref>. This suggests that scaling up vision models also benefits video recognition tasks. In this scenario, using a larger window size at test time can also bring additional benefits of +0.2%, probably attributed to the effective Log-spaced CPB approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>Ablation on res-post-norm and scaled cosine attention Table <ref type="table" coords="8,75.42,396.22,4.98,8.64" target="#tab_6">6</ref> ablates the performance of applying the proposed res-post-norm and scaled cosine attention approaches to Swin Transformer. Both techniques improve the accuracy at all the tiny, small and base size, and the overall improvements are +0.2%, +0.4% and +0.5% respectively, indicating the techniques are more beneficial for larger models. It also turns out to benefit ViT architecture (+0.4%). The proposed normalization approach also performs better than some other normalization methods, as shown in Table <ref type="table" coords="8,266.84,491.86,3.74,8.64" target="#tab_7">7</ref>. More importantly, the combination of post-norm and scaled cosine attention stabilize the training. As shown in Figure <ref type="figure" coords="8,78.44,527.72,3.74,8.64" target="#fig_0">2</ref>, while the activation values at deeper layers for the original Swin Transformer are almost exploded at large (L) size, those of the new version have much milder behavior. On a huge size model, the self-supervised pre-training <ref type="bibr" coords="8,269.77,563.59,16.60,8.64" target="#b71">[72]</ref> diverges using the original Swin Transformer, while it trains well by a Swin Transformer V2 model. proaches perform consistently better than the parameterized position bias approach used in Swin Transformer V1. Compared to the linear-spaced approach, the log-spaced version is marginally better; 3) The larger the change in resolutions between pre-training and fine-tuning, the larger the benefit of the proposed log-spaced CPB approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scaling up window resolution by different approaches</head><p>In Table <ref type="table" coords="8,357.01,251.30,4.98,8.64">1</ref> and 8, we also report the accuracy using targeted window resolutions without fine-tuning (see the first number in each column in the ImageNet-1K experiments). The recognition accuracy remains not bad even when the window size is enlarged from 8 to 24 (78.9% versus 81.8%), while the top-1 accuracy of the original approach significantly degrades from 81.7% to 68.7%. Also note that without fine-tuning, using a window size of 12 that the pretrained model has never seen before can even be +0.4% higher that the original accuracy. This suggests that we can improve accuracy through test-time window adjustment, as also observed in Table <ref type="table" coords="8,400.33,382.81,3.74,8.64" target="#tab_2">3</ref>, 4 and 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented techniques for scaling Swin Transformer up to 3 billion parameters and making it capable of training with images of up to 1,536×1,536 resolution, including the res-post-norm and scaled cosine attention to make the model easier to be scaled up in capacity, as well a log-spaced continuous relative position bias approach which lets the model more effectively transferred across window resolutions. The adapted architecture is named Swin Transformer V2, and by scaling up capacity and resolution, it sets new records on 4 representative vision benchmarks. By these strong results, we hope to stimulate more research in this direction so that we can eventually close the capacity gap between vision and language models and facilitate the joint modeling of the two domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1. Experimental Settings for Ablation</head><p>This section describes the experimental settings for ablation, including models of SwinV2-T, SwinV2-S, and SwinV2-B, and tasks of ImageNet-1K image classification, COCO object detection and ADE semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.1. ImageNet-1K Pre-training</head><p>All ablation study use the ImageNet-1K image classification task for pre-training. We adopt an input image size (window size) of 256×256 (8×8) 2 . Following <ref type="bibr" coords="9,235.63,192.99,15.27,8.64" target="#b45">[46]</ref>, we employ an AdamW <ref type="bibr" coords="9,119.20,204.94,16.60,8.64" target="#b47">[48]</ref> optimizer for 300 epochs using a cosine decay learning rate scheduler with 20 epochs of linear warm-up. A batch size of 1024, an initial learning rate of 1×10 -3 , a weight decay of 0.05, and gradient clipping with a max norm of 5.0 are used. Augmentation and regularization strategies include RandAugment <ref type="bibr" coords="9,215.00,264.72,15.27,8.64" target="#b14">[15]</ref>, Mixup <ref type="bibr" coords="9,267.28,264.72,15.27,8.64" target="#b80">[81]</ref>, Cutmix <ref type="bibr" coords="9,82.07,276.67,15.27,8.64" target="#b78">[79]</ref>, random erasing <ref type="bibr" coords="9,166.62,276.67,16.60,8.64" target="#b83">[84]</ref> and stochastic depth <ref type="bibr" coords="9,267.28,276.67,15.27,8.64" target="#b31">[32]</ref>. An increasing degree of stochastic depth augmentation is employed for larger models, i.e. 0.2, 0.3, 0.5 for tiny, small, and base models, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.2. Fine-tuning on various tasks</head><p>ImageNet-1K image classification For ImageNet-1K image classification experiments, we conduct a fine-tuning step if the input image resolution is larger than that in the pre-training step. The fine-tuning lasts for 30 epochs, with an AdamW <ref type="bibr" coords="9,102.04,398.98,16.60,8.64" target="#b47">[48]</ref> optimizer, a cosine decay learning rate scheduler with an initial learning rate of 4 × 10 -5 , a weight decay of 1 × 10 -8 , and the same data augmentation and regularizations as those in the first stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COCO object detection</head><p>We use cascade mask R-CNN <ref type="bibr" coords="9,73.30,474.37,12.40,8.64" target="#b7">[8,</ref><ref type="bibr" coords="9,85.69,474.37,12.40,8.64" target="#b27">28]</ref> implemented in mmdetection <ref type="bibr" coords="9,218.49,474.37,16.60,8.64" target="#b10">[11]</ref> as the object detection framework. In training, a multi-scale augmentation <ref type="bibr" coords="9,68.57,498.28,10.79,8.64" target="#b8">[9,</ref><ref type="bibr" coords="9,80.98,498.28,13.28,8.64" target="#b60">61]</ref> with the shorter side between 480 and 800 and the longer side of 1333 is used. The window size is set 16×16. An AdamW <ref type="bibr" coords="9,135.19,522.19,16.60,8.64" target="#b47">[48]</ref> optimizer with an initial learning rate of 1 × 10 -4 , a weight decay of 0.05, a batch size of 16, and a 3× scheduler are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ADE20K semantic segmentation</head><p>We adopt an image size (window size) of 512×512 (16×16). In training, we employ an AdamW <ref type="bibr" coords="9,132.62,597.57,16.60,8.64" target="#b47">[48]</ref> optimizer with an initial learning rate of 4 × 10 -5 , a weight decay of 0.05, a learning rate scheduler that uses linear learning rate decay and a linear warm-up of 1,500 iterations. Models are trained with batch size of 16 for 160K iterations. We follow the mmsegmentation codebase to adopt augmentations of random horizontal 2 Most of our experiments have the window size as an even number to make the window shifting offset divisible by the window size. Nevertheless, an odd number of window size also works well, as is right the case in the original Swin Transformer (7 × 7).</p><p>flipping, random re-scaling within ratio range [0.5, 2.0] and a random photometric distortion. Stochastic depth with ratio of 0.3 is applied for all models. A layer-wise learning rate decay <ref type="bibr" coords="9,352.01,111.34,11.62,8.64" target="#b3">[4]</ref> of 0.95 is adopted for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2. Experimental Settings for System-Level Comparison</head><p>A2.1. SwinV2-B and SwinV2-L Settings ImageNet-22K pre-training Both models use an input image size (window size) of 192×192 (12×12). We employ an AdamW optimizer <ref type="bibr" coords="9,420.90,286.60,16.60,8.64" target="#b47">[48]</ref> for 90 epochs using a cosine learning rate scheduler with 5-epoch linear warm-up. A batch size of 4096, an initial learning rate of 0.001, a weight decay of 0.1, and gradient clipping with a max norm of 5.0 are used. Augmentation and regularization strategies include RandAugment <ref type="bibr" coords="9,401.69,346.38,15.27,8.64" target="#b14">[15]</ref>, Mixup <ref type="bibr" coords="9,452.80,346.38,15.27,8.64" target="#b80">[81]</ref>, Cutmix <ref type="bibr" coords="9,507.24,346.38,15.27,8.64" target="#b78">[79]</ref>, random erasing <ref type="bibr" coords="9,359.65,358.33,16.60,8.64" target="#b83">[84]</ref> and stochastic depth <ref type="bibr" coords="9,460.68,358.33,16.60,8.64" target="#b31">[32]</ref> with ratio of 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet-1K image classification</head><p>We consider input image sizes of 256×256 and 384×384. The training length is set 30 epochs, with a batch size of 1024, a cosine decay learning rate scheduler with an initial learning rate of 4 × 10 -5 , and a weight decay of 1 × 10 -8 . The ImageNet-1K classification weights are also initialized from the corresponding ones in the ImageNet-22K model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COCO object detection</head><p>We adopt HTC++ <ref type="bibr" coords="9,498.56,485.51,15.77,8.64" target="#b9">[10,</ref><ref type="bibr" coords="9,516.46,485.51,13.28,8.64" target="#b45">46]</ref> for experiments. In data pre-processing, Instaboost <ref type="bibr" coords="9,516.29,497.47,15.27,8.64" target="#b22">[23]</ref>, a multi-scale training <ref type="bibr" coords="9,396.29,509.42,16.60,8.64" target="#b25">[26]</ref> with an input image size of 1536×1536, a window size of 32×32, and a random scale between [0.1, 2.0] are used. An AdamW optimizer <ref type="bibr" coords="9,508.89,533.33,16.60,8.64" target="#b47">[48]</ref> with an initial learning rate of 4 × 10 -4 on batch size of 64, a weight decay of 0.05, and a 3× scheduler are used. The backbone learning rate is set 0.1× of the head learning rate. In inference, soft-NMS <ref type="bibr" coords="9,406.64,581.15,11.62,8.64" target="#b4">[5]</ref> is used. Both single-scale and multi-scale test results are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ADE20K semantic segmentation</head><p>The input image size (window size) is set 640×640 (40×40). We employ an AdamW <ref type="bibr" coords="9,347.80,644.74,16.60,8.64" target="#b47">[48]</ref> optimizer with an initial learning rate of 6 × 10 -5 , a weight decay of 0.05, a linear decayed learning rate scheduler with 375-iteration linear warm-up. The model is trained with batch size of 64 for 40K iterations. We follow the default settings in mmsegmentation for data augmentation, including random horizontal flipping, random re-scaling within ratio range [0.5, 2.0] and random photometric distortion. Stochastic depth with ratio of 0.3 is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.2. SwinV2-G Settings</head><p>Stage-1 self-supervised pre-training The model is first pre-trained using a self-supervised learning approach <ref type="bibr" coords="10,274.75,148.15,11.62,8.64" target="#b0">[1]</ref> on the ImageNet-22K-ext dataset (70 million images) for 20 epochs. To reduce experimental overheads, we adopt a smaller image size of 192×192. The model is trained using the AdamW <ref type="bibr" coords="10,99.85,195.97,16.60,8.64" target="#b47">[48]</ref> optimizer with a cosine decay learning rate scheduler with 30000 steps of linear warm-up. A batch size of 9216, an initial learning rate of 1.4 × 10 -3 , a weight decay of 0.1, and gradient clipping with a max norm of 100.0 are used. A light data augmentation strategy is employed: random resize cropping with scale range of [0.67, 1] and a aspect ratio range of [3/4, 4/3], followed by a random flipping and a color normalization steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage-2 supervised pre-training</head><p>The model is further pre-trained using the class labels on the ImageNet-22Kext dataset. We employ an AdamW <ref type="bibr" coords="10,200.15,329.62,16.60,8.64" target="#b47">[48]</ref> optimizer for 30 epochs, using a cosine decayed learning rate scheduler with 20000 steps of linear warm-up. A batch size of 9216, an initial learning rate of 1.4 × 10 -3 , a layer-wise learning rate decay of 0.87, a weight decay of 0.1, and gradient clipping with a max norm of 100.0 are used. Augmentation and regularization strategies include RandAugment <ref type="bibr" coords="10,232.75,401.35,15.27,8.64" target="#b14">[15]</ref>, random erasing <ref type="bibr" coords="10,81.37,413.31,16.60,8.64" target="#b83">[84]</ref> and a stochastic depth <ref type="bibr" coords="10,190.66,413.31,16.60,8.64" target="#b31">[32]</ref> ratio of 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-tuning on ImageNet-1K image classification</head><p>We adopt an input image size of 640×640 for experiments. An AdamW <ref type="bibr" coords="10,86.55,463.27,16.60,8.64" target="#b47">[48]</ref> optimizer is employed for 10 epochs, using a cosine decayed learning rate scheduler and a 2-epoch linear warm-up. A batch size of 576, an initial learning rate of 2.1 × 10 -5 , a weight decay of 0.1, and gradient clipping with a max norm of 100.0 are used. Augmentation and regularization strategies include RandAugment <ref type="bibr" coords="10,232.75,523.04,15.27,8.64" target="#b14">[15]</ref>, random erasing <ref type="bibr" coords="10,81.37,535.00,16.60,8.64" target="#b83">[84]</ref> and a stochastic depth <ref type="bibr" coords="10,190.66,535.00,16.60,8.64" target="#b31">[32]</ref> ratio of 0.5.</p><p>In evaluation, we test top-1 accuracy on both ImageNet-1K V1 and V2.</p><p>Fine-tuning on COCO object detection We first conduct inter-mediate fine-tuning using the Objects-365 V2 dataset. In this stage, we remove the mask branch of the HTC++ framework <ref type="bibr" coords="10,130.57,620.83,15.77,8.64" target="#b9">[10,</ref><ref type="bibr" coords="10,147.95,620.83,13.28,8.64" target="#b45">46]</ref> because there are no mask annotations. The input image resolution and window size are set as [800, 1024] and 32 × 32, respectively. In training, an AdamW <ref type="bibr" coords="10,117.31,656.69,16.60,8.64" target="#b47">[48]</ref> optimizer with initial learning rate of 1.2 × 10 -3 , a weight decay of 0.05 and a batch size of 96 are used, and the training length is set 67,500 steps.</p><p>Then we fine-tune the HTC++ model on COCO dataset, with the mask branch randomly initialized and other model weights loaded from the Objects-365-V2 pre-trained model. In this training stage, the input image resolution is set 1536×1536 with a multi-scale ratio of [0.1, 2.0]. The window size is set 32×32. The AdamW <ref type="bibr" coords="10,459.64,111.34,16.60,8.64" target="#b47">[48]</ref> optimizer is employed, with an initial learning rate of 6 × 10 -4 , a weight decay of 0.05, and a batch size of 96, and is trained 45,000 steps.</p><p>In test, Soft-NMS <ref type="bibr" coords="10,393.29,160.17,11.62,8.64" target="#b4">[5]</ref> is used. Both window sizes of 32× 32 and 48 × 48 are considered.</p><p>Fine-tuning on ADE20K semantic segmentation The input image size (window size) is set 640×640 (40×40). An AdamW optimizer <ref type="bibr" coords="10,406.60,228.09,16.60,8.64" target="#b47">[48]</ref> is employed, with an initial learning rate of 4 × 10 -5 , a weight decay of 0.05, a linear decayed learning rate scheduler with 80K iterations, a batch size of 32, and a linear warm-up of 750 iterations. For augmentations, we follow the default settings in mmsegmentation to include random horizontal flipping, random re-scaling within ratio range [0.5, 2.0] and random photometric distortion. The stochastic depth ratio is set 0.4.</p><p>Fine-tuning on Kinetics-400 video action recognition A 2-stage fine-tuning process is employed. In the first stage, an input resolution of 256×256×8 with 16×16×8 window size is adopted. We employ the AdamW optimizer for 20 epochs using a cosine decayed learning rate scheduler with 2.5-epoch linear warm-up. Other training hyper-parameters are: batch-size 80, an initial learning rate of 3.6×10 -4 , and a weight decay of 0.1.</p><p>In the second stage, we further fine-tune the model using a larger input video resolution of 320×320×8 with 20×20×8 window size. We employ the AdamW optimizer for 5 epochs using a cosine decayed learning rate scheduler with 1-epoch linear warm-up. A batch-size of 64, an initial learning rate of 5 × 10 -5 and a weight decay of 0.1 are set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3. Learnt Relative Position Bias by Different Approaches</head><p>Figure <ref type="figure" coords="10,351.18,561.05,4.98,8.64">4</ref> visualizes the relative position bias matrices ( B ∈ R (2M -1)×(2M -1) ) learnt by different bias computation approaches, using a SwinV2-T model. The bias matrices of the 3 heads in the first block are visualized. The left shows the bias matrices learnt by using an input image size of 256×256 and a window size of 8 × 8. The right shows the bias matrices after fine-tuning on a larger input image resolution of 512×512 and a larger window size of 16×16. It turns out that the bias matrices learnt by two CPB(continuous position bias) approaches are more smoothly than that learnt by P-RPE (parameterized relative position bias). Figure <ref type="figure" coords="10,396.37,692.56,4.98,8.64">5</ref> shows more examples using the last block of this model.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,50.11,371.92,236.25,7.77;4,50.11,382.88,236.25,7.77;4,50.11,393.84,236.25,7.77;4,50.11,404.80,210.91,7.77"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The Signal Propagation Plot [6, 76] for various model sizes. H-size models are trained at a self-supervised learning phase, and other sizes are trained by an image classification task. * indicates that we use a 40-epoch model before it crashes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,68.79,220.82,198.89,7.77"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. SwinV1-H versus SwinV2-H in training [72].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="11,50.11,309.02,495.00,7.77;11,50.11,319.98,495.00,7.77;11,50.11,330.94,296.30,7.77;11,101.21,227.06,210.49,70.16"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Figure 4. Visualization of the learnt relative position bias matrices by different approaches, using a SwinV2-T model and the 3 heads in the first block. Left: the bias matrices by pre-training on a 256×256 image and a 8×8 window; Right: the bias matrices after fine-tuning using a 512×512 image size and 16×16 window size. H-x indicates the x-th head.</figDesc><graphic coords="11,101.21,227.06,210.49,70.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,58.69,73.06,477.84,87.74"><head></head><label></label><figDesc>/82.7 77.2/83.0 73.2/83.2 68.7/83.2 50.8 50.9 45.5 45.8 44.5</figDesc><table coords="4,58.69,73.06,477.84,87.74"><row><cell></cell><cell>ImageNet*</cell><cell></cell><cell cols="2">ImageNet  †</cell><cell></cell><cell cols="2">COCO</cell><cell cols="2">ADE20k</cell><cell></cell></row><row><cell>method</cell><cell>W8, I256 top-1 acc</cell><cell>W12, I384 top-1 acc</cell><cell>W16, I512 top-1 acc</cell><cell>W20, I640 top-1 acc</cell><cell>W24, I768 top-1 acc</cell><cell>W16 AP box</cell><cell>W32 AP box</cell><cell>W16 mIoU</cell><cell>W20 mIoU</cell><cell>W32 mIoU</cell></row><row><cell cols="3">Parameterized position bias [46] 79.4Linear-Spaced CPB 81.7 81.7 82.0/82.9 (+0.0) (+2.6/+0.2)</cell><cell>81.2/83.3 (+4.0/+0.3)</cell><cell>79.8/83.6 (+6.6/+0.4)</cell><cell>77.6/83.7 (+8.9/+0.5)</cell><cell>50.9 (+0.1)</cell><cell>51.7 (+0.8)</cell><cell>47.0 (+1.5)</cell><cell>47.4 (+1.6)</cell><cell>47.2 (+2.7)</cell></row><row><cell>Log-Spaced CPB</cell><cell>81.8 (+0.1)</cell><cell>82.4/83.2 (+3.0/+0.5)</cell><cell>81.7/83.8 (+4.5/+0.8)</cell><cell>80.4/84.0 (+7.2/+0.8)</cell><cell>79.1/84.2 (+10.4/+1.0)</cell><cell>51.1 (+0.3)</cell><cell>51.8 (+0.9)</cell><cell>47.0 (+1.5)</cell><cell>47.7 (+1.9)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,50.11,74.83,495.00,319.46"><head>Table 2 .</head><label>2</label><figDesc>Comparison with previous largest vision models on ImageNet-1K V1 and V2 classification. * indicates the sparse model; the "pre-train time" column is measured by the TPUv3 core days with numbers copied from the original papers. † That of SwinV2-G is estimated according to training iterations and FLOPs.</figDesc><table coords="7,58.97,74.83,475.19,319.46"><row><cell>Method</cell><cell>param</cell><cell cols="2">pre-train images</cell><cell>pre-train length (#im)</cell><cell>pre-train im size</cell><cell>pre-train time</cell><cell>fine-tune im size</cell><cell>ImageNet-1K-V1 top-1 acc</cell><cell>ImaegNet-1K-V2 top-1 acc</cell></row><row><cell>SwinV1-B</cell><cell>88M</cell><cell cols="2">IN-22K-14M</cell><cell>1.3B</cell><cell>224 2</cell><cell>&lt;30  †</cell><cell>384 2</cell><cell>86.4</cell><cell>76.58</cell></row><row><cell>SwinV1-L</cell><cell>197M</cell><cell cols="2">IN-22K-14M</cell><cell>1.3B</cell><cell>224 2</cell><cell>&lt;10  †</cell><cell>384 2</cell><cell>87.3</cell><cell>77.46</cell></row><row><cell>ViT-G [80]</cell><cell>1.8B</cell><cell cols="2">JFT-3B</cell><cell>164B</cell><cell>224 2</cell><cell>&gt;30k</cell><cell>518 2</cell><cell>90.45</cell><cell>83.33</cell></row><row><cell>V-MoE [56]</cell><cell>14.7B*</cell><cell cols="2">JFT-3B</cell><cell>-</cell><cell>224 2</cell><cell>16.8k</cell><cell>518 2</cell><cell>90.35</cell><cell>-</cell></row><row><cell cols="2">CoAtNet-7 [17] 2.44B</cell><cell cols="2">JFT-3B</cell><cell>-</cell><cell>224 2</cell><cell>20.1k</cell><cell>512 2</cell><cell>90.88</cell><cell>-</cell></row><row><cell>SwinV2-B</cell><cell>88M</cell><cell cols="2">IN-22K-14M</cell><cell>1.3B</cell><cell>192 2</cell><cell>&lt;30  †</cell><cell>384 2</cell><cell>87.1</cell><cell>78.08</cell></row><row><cell>SwinV2-L</cell><cell>197M</cell><cell cols="2">IN-22K-14M</cell><cell>1.3B</cell><cell>192 2</cell><cell>&lt;20  †</cell><cell>384 2</cell><cell>87.7</cell><cell>78.31</cell></row><row><cell>SwinV2-G</cell><cell cols="3">3.0B IN-22K-ext-70M</cell><cell>3.5B</cell><cell>192 2</cell><cell>&lt;0.5k  †</cell><cell>640 2</cell><cell>90.17</cell><cell>84.00</cell></row><row><cell>Method</cell><cell>train I(W) size</cell><cell>test I(W) size</cell><cell cols="2">mini-val (AP) test-dev (AP) box mask box mask</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">CopyPaste [25] 1280(-) 1280(-) 57.0 48.9 57.3 49.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SwinV1-L [46] 800(7)</cell><cell cols="3">ms(7) 58.0 50.4 58.7 51.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">YOLOR [66] 1280(-) 1280(-)</cell><cell>-</cell><cell>-57.3 -</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CBNet [43]</cell><cell>1400(7)</cell><cell cols="3">ms(7) 59.6 51.8 60.1 52.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">DyHead [16] 1200(-)</cell><cell cols="3">ms(-) 60.3 -60.6 -</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">SoftTeacher [74] 1280(12) ms(12) 60.7 52.5 61.3 53.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SwinV2-L (HTC++)</cell><cell>1536(32)</cell><cell cols="3">1100(32) 58.8 51.1 -1100 (48) 58.9 51.2 -ms (48) 60.2 52.1 60.8 52.7 --</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SwinV2-G (HTC++)</cell><cell>1536(32)</cell><cell cols="3">1100(32) 61.7 53.3 -1100 (48) 61.9 53.4 -ms (48) 62.5 53.7 63.1 54.4 --</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,50.11,408.43,236.25,178.45"><head>Table 3 .</head><label>3</label><figDesc>Comparison with previous best results on COCO object detection and instance segmentation. I(W) indicates the image and window size. ms indicate multi-scale testing is employed.</figDesc><table coords="7,69.43,457.31,197.62,129.57"><row><cell>Method</cell><cell cols="3">train I(W) size test I(W) size mIoU</cell></row><row><cell>SwinV1-L [46]</cell><cell>640(7)</cell><cell>640(7)</cell><cell>53.5*</cell></row><row><cell>Focal-L [75]</cell><cell>640(40)</cell><cell>640(40)</cell><cell>55.4*</cell></row><row><cell>CSwin-L [21]</cell><cell>640(40)</cell><cell>640(40)</cell><cell>55.7*</cell></row><row><cell>MaskFormer [13]</cell><cell>640(7)</cell><cell>640(7)</cell><cell>55.6*</cell></row><row><cell>FaPN [33]</cell><cell>640(7)</cell><cell>640(7)</cell><cell>56.7*</cell></row><row><cell>BEiT [4]</cell><cell>640(40)</cell><cell>640(40)</cell><cell>58.4*</cell></row><row><cell>SwinV2-L (UperNet)</cell><cell>640(40)</cell><cell>640(40)</cell><cell>55.9*</cell></row><row><cell>SwinV2-G (UperNet)</cell><cell>640(40)</cell><cell>640(40) 896 (56) 896 (56)</cell><cell>59.1 59.3 59.9*</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,50.11,601.02,236.25,18.73"><head>Table 4 .</head><label>4</label><figDesc>Comparison with previous best results on ADE20K semantic segmentation. * indicates multi-scale testing is used.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,308.86,242.85,243.36,107.55"><head>Table 5 .</head><label>5</label><figDesc>Comparison with previous best results on Kinetics-400 video action classification.</figDesc><table coords="7,310.85,242.85,241.37,74.67"><row><cell>Method</cell><cell cols="3">train I(W) size test I(W) size views top-1</cell></row><row><cell>ViViT [2]</cell><cell>-(-)</cell><cell>-(-)</cell><cell>4×3 84.8</cell></row><row><cell cols="4">SwinV1-L [47] 480(12) 2 ×16(8) 480(12) 2 ×16(8) 10×5 84.9</cell></row><row><cell cols="4">TokenLearner [57] 256(8) 2 ×64(64) 256(8) 2 ×64(64) 4×3 85.4</cell></row><row><cell></cell><cell></cell><cell cols="2">320(20) 2 ×8(8) 1×1 83.2</cell></row><row><cell cols="2">Video-SwinV2-G 320(20) 2 ×8(8)</cell><cell cols="2">384(24) 2 ×8(8) 1×1 83.4</cell></row><row><cell></cell><cell></cell><cell cols="2">384(24) 2 ×8(8) 4×5 86.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,308.86,367.47,236.25,56.46"><head>Table 3</head><label>3</label><figDesc></figDesc><table coords="7,490.36,367.47,54.75,8.64"><row><cell>compares the</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,56.50,74.83,223.47,202.12"><head>Table 6 .</head><label>6</label><figDesc>Ablation on res-post-norm and cosine attention.</figDesc><table coords="8,56.50,74.83,223.47,202.12"><row><cell cols="3">Backbone res-post-norm</cell><cell>scaled cosine attention</cell><cell cols="2">ImageNet top-1 acc</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>81.5</cell></row><row><cell>Swin-T</cell><cell></cell><cell></cell><cell></cell><cell>81.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>81.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>83.2</cell></row><row><cell>Swin-S</cell><cell></cell><cell></cell><cell></cell><cell>83.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>83.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>83.6</cell></row><row><cell>Swin-B</cell><cell></cell><cell></cell><cell></cell><cell>83.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>84.1</cell></row><row><cell>ViT-B</cell><cell></cell><cell></cell><cell></cell><cell>82.2 82.6</cell></row><row><cell cols="6">Backbone pre-norm sandwich [20] post-norm [65] our</cell></row><row><cell>Swin-S</cell><cell>83.2</cell><cell>82.6</cell><cell cols="2">83.3</cell><cell>83.6</cell></row><row><cell>Swin-B</cell><cell>83.6</cell><cell>-</cell><cell cols="2">83.6</cell><cell>84.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="8,50.11,291.09,236.25,40.65"><head>Table 7 .</head><label>7</label><figDesc>Comparison with other normalization methods. The postnorm method diverges at the default learning rate, and we use 1/4 of the default learning rate for this method. Sandwich performs worse than ours, probably because it sacrifices expressiveness.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="8,50.11,73.06,482.77,640.09"><head>Table 8 .</head><label>8</label><figDesc>Table1and 8 ablate the performance of 3 approaches by scaling window resolutions from 256 × 256 in pretraining to larger sizes in 3 down-stream vision tasks of ImageNet-1K image classification, COCO object detection, and ADE20K semantic segmentation, respectively. It can be seen that: 1) Different approaches have similar accuracy in pre-training (81.7%-81.8%); 2) When transferred to downstream tasks, the two continuous position bias (CPB) ap-Ablation on Log-CPB using different model sizes.</figDesc><table coords="8,322.09,73.06,209.79,65.47"><row><cell></cell><cell>ImageNet*</cell><cell>ImageNet  †</cell></row><row><cell cols="3">Backbone L-CPB W8, I256 W12, I384 W16, I512</cell></row><row><cell>SwinV2-S</cell><cell>83.7 83.7</cell><cell>81.8/84.5 79.4/84.9 84.1/84.8 82.9/85.4</cell></row><row><cell>SwinV2-B</cell><cell>84.1 84.2</cell><cell>82.9/85.0 81.0/85.3 84.5/85.1 83.8/85.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="9,308.86,187.15,236.25,56.46"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="9,308.86,187.15,236.25,56.46"><row><cell>, 3 and 4 include results of SwinV2-B and</cell></row><row><cell>SwinV2-L. For these experiments, we first conduct</cell></row><row><cell>ImageNet-22K pre-training, and then fine-tune the pre-</cell></row><row><cell>trained models on individual down-stream recognition</cell></row><row><cell>tasks.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,323.21,677.42,221.90,6.91;3,308.86,686.88,236.25,6.91;3,308.86,696.35,236.25,6.91;3,308.86,705.81,221.25,6.91"><p>There have been a few alternative normalization configurations, such as post-normalization<ref type="bibr" coords="3,385.06,686.88,13.28,6.91" target="#b64">[65]</ref> and sandwich normalization<ref type="bibr" coords="3,502.93,686.88,12.22,6.91" target="#b19">[20]</ref>. Postnormalization harms training stability<ref type="bibr" coords="3,434.88,696.35,12.22,6.91" target="#b72">[73]</ref>, and sandwich normalization sacrifices representation power due to too many normalization layers.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>We thank many colleagues at <rs type="institution">Microsoft</rs> for their help, in particular, <rs type="person">Eric Chang</rs>, <rs type="person">Lidong Zhou</rs>, <rs type="person">Jing Tao</rs>, <rs type="person">Aaron Zhang</rs>, <rs type="person">Edward Cui</rs>, <rs type="person">Bin Xiao</rs>, <rs type="person">Lu Yuan</rs>, <rs type="person">Peng Cheng</rs>, <rs type="person">Fan Yang</rs> for useful discussion and the help on GPU resources and datasets.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="11,70.04,383.21,216.32,7.77;11,70.03,394.01,165.11,7.93" xml:id="b0">
	<analytic>
		<title level="a" type="main" coords="11,119.25,383.21,167.11,7.77;11,70.03,394.17,46.79,7.77">Simmim: A simple framework for masked image modeling</title>
		<author>
			<persName coords=""><surname>Anonymous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="11,133.77,394.01,62.93,7.73">CVPR submission</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,70.04,407.28,216.32,7.77;11,70.03,418.24,216.33,7.77;11,70.03,429.20,99.99,7.77" xml:id="b1">
	<monogr>
		<title level="m" type="main" coords="11,221.35,418.24,65.01,7.77;11,70.03,429.20,57.26,7.77">Vivit: A video vision transformer</title>
		<author>
			<persName coords=""><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mario</forename><surname>Lučić</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,70.04,442.32,216.32,7.77;11,70.03,453.28,105.33,7.77" xml:id="b2">
	<monogr>
		<title level="m" type="main" coords="11,70.03,453.28,71.48,7.77">Layer normalization</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,70.04,466.39,216.32,7.77;11,70.03,477.35,129.48,7.77" xml:id="b3">
	<monogr>
		<title level="m" type="main" coords="11,206.48,466.39,79.88,7.77;11,70.03,477.35,77.77,7.77">Beit: Bert pre-training of image transformers</title>
		<author>
			<persName coords=""><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,70.04,490.47,216.32,7.77;11,70.03,501.43,216.33,7.77;11,70.03,512.23,216.33,7.93;11,70.03,523.19,204.46,7.93" xml:id="b4">
	<analytic>
		<title level="a" type="main" coords="11,128.87,501.43,157.50,7.77;11,70.03,512.39,57.42,7.77">Soft-nms -improving object detection with one line of code</title>
		<author>
			<persName coords=""><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="11,146.31,512.23,140.05,7.73;11,70.03,523.19,141.60,7.73">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10">Oct 2017</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,70.04,536.46,216.32,7.77;11,70.03,547.42,216.33,7.77;11,70.03,558.22,216.33,7.93;11,70.03,569.34,4.48,7.77" xml:id="b5">
	<monogr>
		<title level="m" type="main" coords="11,248.70,536.46,37.66,7.77;11,70.03,547.42,216.33,7.77;11,70.03,558.38,66.28,7.77">Characterizing signal propagation to close the performance gap in unnormalized resnets</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samuel</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.08692</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,70.04,582.46,216.32,7.77;11,70.03,593.42,216.33,7.77;11,70.03,604.37,216.33,7.77;11,70.03,615.33,216.33,7.77;11,70.03,626.29,216.33,7.77;11,70.03,637.25,216.33,7.77;11,70.03,648.21,216.33,7.77;11,70.03,659.17,216.33,7.77;11,70.03,670.13,216.33,7.77;11,70.03,681.09,116.29,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coords="11,222.24,670.13,64.12,7.77;11,70.03,681.09,73.80,7.77">Language models are few-shot learners</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="11,90.22,670.13,122.37,7.77">Ilya Sutskever, and Dario Amodei</title>
		<meeting><address><addrLine>Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,70.04,694.20,216.32,7.77;11,70.03,705.00,216.33,7.93;11,328.78,363.95,216.33,7.73;11,328.78,374.91,113.81,7.93" xml:id="b7">
	<analytic>
		<title level="a" type="main" coords="11,208.48,694.20,77.88,7.77;11,70.03,705.16,131.91,7.77">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName coords=""><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="11,219.16,705.00,67.20,7.73;11,328.78,363.95,216.33,7.73;11,328.78,374.91,12.95,7.73">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,328.79,386.22,216.32,7.77;11,328.78,397.18,216.33,7.77;11,328.78,407.98,216.34,7.93;11,328.78,418.94,216.33,7.93;11,328.78,430.06,4.48,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coords="11,517.72,397.18,27.39,7.77;11,328.78,408.14,135.70,7.77">End-toend object detection with transformers</title>
		<author>
			<persName coords=""><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="11,480.41,407.98,64.71,7.73;11,328.78,418.94,92.14,7.73">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,328.79,441.20,216.32,7.77;11,328.78,452.16,216.33,7.77;11,328.78,463.12,216.33,7.77;11,328.78,473.92,216.33,7.93;11,328.78,484.88,216.33,7.93;11,328.78,496.00,63.75,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coords="11,426.40,463.12,118.71,7.77;11,328.78,474.08,46.08,7.77">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="11,392.82,473.92,152.29,7.73;11,328.78,484.88,163.49,7.73">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,328.79,507.15,216.32,7.77;11,328.78,518.10,216.33,7.77;11,328.78,529.06,216.33,7.77;11,328.78,539.86,216.33,7.93;11,328.78,550.98,4.48,7.77" xml:id="b10">
	<monogr>
		<title level="m" type="main" coords="11,389.37,529.06,155.75,7.77;11,328.78,540.02,67.76,7.77">Mmdetection: Open mmlab detection toolbox and benchmark</title>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,328.79,562.13,216.32,7.77;11,328.78,573.09,209.87,7.77" xml:id="b11">
	<monogr>
		<title level="m" type="main" coords="11,328.78,573.09,167.44,7.77">Training deep nets with sublinear memory cost</title>
		<author>
			<persName coords=""><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2016. 2, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,328.79,584.24,216.32,7.77;11,328.78,595.20,216.33,7.77;11,328.78,605.99,105.44,7.93" xml:id="b12">
	<monogr>
		<title level="m" type="main" coords="11,349.70,595.20,195.41,7.77;11,328.78,606.15,46.08,7.77">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName coords=""><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<idno>arXiv, 2021. 7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,328.79,617.30,216.32,7.77;11,328.78,628.26,216.33,7.77;11,328.78,639.22,216.33,7.77;11,328.78,650.18,77.68,7.77" xml:id="b13">
	<monogr>
		<title level="m" type="main" coords="11,328.78,639.22,216.33,7.77;11,328.78,650.18,43.90,7.77">Twins: Revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName coords=""><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haibing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,328.79,661.33,216.32,7.77;11,328.78,672.29,216.33,7.77;11,328.78,683.08,216.33,7.93;11,328.78,694.04,216.33,7.73;11,328.78,705.00,188.85,7.93" xml:id="b14">
	<analytic>
		<title level="a" type="main" coords="11,350.77,672.29,194.35,7.77;11,328.78,683.24,122.88,7.77">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName coords=""><forename type="first">Barret</forename><surname>Ekin D Cubuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="11,475.06,683.08,70.05,7.73;11,328.78,694.04,216.33,7.73;11,328.78,705.00,82.62,7.73">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,70.04,640.92,216.32,7.77;12,70.03,651.88,216.33,7.77;12,70.03,662.84,201.97,7.77" xml:id="b15">
	<monogr>
		<title level="m" type="main" coords="12,230.72,651.88,55.65,7.77;12,70.03,662.84,168.47,7.77">Dynamic head: Unifying object detection heads with attentions</title>
		<author>
			<persName coords=""><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,70.04,683.24,216.32,7.77;12,70.03,694.20,216.33,7.77;12,70.03,705.16,85.66,7.77" xml:id="b16">
	<monogr>
		<title level="m" type="main" coords="12,70.03,694.20,216.33,7.77;12,70.03,705.16,16.39,7.77">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName coords=""><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2021. 1, 2, 3, 5, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,328.79,640.92,216.32,7.77;12,328.78,651.88,216.33,7.77;12,328.78,662.68,216.33,7.93;12,328.78,673.64,179.37,7.93" xml:id="b17">
	<analytic>
		<title level="a" type="main" coords="12,388.01,651.88,157.10,7.77;12,328.78,662.84,29.43,7.77">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="12,375.97,662.68,169.14,7.73;12,328.78,673.64,67.71,7.73">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,328.79,694.20,216.32,7.77;12,328.78,705.16,39.51,7.77;12,384.63,705.16,160.48,7.77;13,70.03,75.96,216.33,7.94;13,70.03,86.92,106.83,7.94" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coords="12,413.54,705.16,131.57,7.77;13,70.03,76.13,149.94,7.77">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,70.04,99.22,216.32,7.77;13,70.03,110.18,216.33,7.77;13,70.03,121.14,216.33,7.77;13,70.03,132.10,146.18,7.77;13,231.40,131.94,54.96,7.73;13,70.03,142.90,106.83,7.93" xml:id="b19">
	<monogr>
		<title level="m" type="main" coords="13,189.07,121.14,97.29,7.77;13,70.03,132.10,142.52,7.77">Cogview: Mastering textto-image generation via transformers</title>
		<author>
			<persName coords=""><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhou</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.13290</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,70.04,155.20,216.32,7.77;13,70.03,166.16,216.33,7.77;13,70.03,177.12,216.33,7.77;13,70.03,188.07,140.23,7.77" xml:id="b20">
	<monogr>
		<title level="m" type="main" coords="13,70.03,177.12,216.33,7.77;13,70.03,188.07,96.83,7.77">Cswin transformer: A general vision transformer backbone with cross-shaped windows</title>
		<author>
			<persName coords=""><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,70.04,200.21,216.32,7.77;13,70.03,211.17,216.33,7.77;13,70.03,222.13,216.33,7.77;13,70.03,233.09,216.33,7.77;13,70.03,244.05,216.33,7.77;13,70.03,254.85,216.33,7.93;13,70.03,265.81,58.77,7.93" xml:id="b21">
	<analytic>
		<title level="a" type="main" coords="13,242.81,233.09,43.55,7.77;13,70.03,244.05,216.33,7.77;13,70.03,255.01,16.80,7.77">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName coords=""><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="13,103.70,254.85,182.66,7.73;13,70.03,265.81,16.40,7.73">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,70.04,278.11,216.32,7.77;13,70.03,289.06,216.33,7.77;13,70.03,300.02,216.33,7.77;13,70.03,310.82,216.33,7.93;13,70.03,321.78,190.76,7.93" xml:id="b22">
	<analytic>
		<title level="a" type="main" coords="13,207.78,289.06,78.58,7.77;13,70.03,300.02,216.33,7.77;13,70.03,310.98,24.63,7.77">Instaboost: Boosting instance segmentation via probability map guided copypasting</title>
		<author>
			<persName coords=""><forename type="first">Jianhua</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Runzhong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minghao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yong-Lu</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cewu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="13,110.18,310.82,176.19,7.73;13,70.03,321.78,98.61,7.73">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="682" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,70.04,334.08,216.32,7.77;13,70.03,345.04,216.33,7.77;13,70.03,356.00,128.43,7.77" xml:id="b23">
	<monogr>
		<title level="m" type="main" coords="13,261.45,334.08,24.91,7.77;13,70.03,345.04,216.33,7.77;13,70.03,356.00,86.06,7.77">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,70.04,368.14,216.32,7.77;13,70.03,379.09,216.33,7.77;13,70.03,390.05,216.33,7.77;13,70.03,400.85,204.49,7.93" xml:id="b24">
	<monogr>
		<title level="m" type="main" coords="13,260.95,379.09,25.41,7.77;13,70.03,390.05,216.33,7.77;13,70.03,401.01,46.08,7.77">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName coords=""><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ekin</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07177</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,70.04,413.15,216.32,7.77;13,70.03,424.11,216.33,7.77;13,70.03,434.91,216.33,7.93;13,70.03,445.87,216.33,7.93;13,70.03,456.99,50.30,7.77" xml:id="b25">
	<analytic>
		<title level="a" type="main" coords="13,70.03,424.11,216.33,7.77;13,70.03,435.07,31.60,7.77">Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName coords=""><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Nas-Fpn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="13,126.85,434.91,159.51,7.73;13,70.03,445.87,163.49,7.73">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7036" to="7045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,70.04,469.13,216.32,7.77;13,70.03,480.08,216.33,7.77;13,70.03,491.04,216.33,7.77;13,70.03,502.00,216.33,7.77;13,70.03,512.96,36.85,7.77" xml:id="b26">
	<monogr>
		<title level="m" type="main" coords="13,70.03,502.00,212.70,7.77">Self-supervised pretraining of visual features in the wild</title>
		<author>
			<persName coords=""><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Lefaudeux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pengchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vivek</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,70.04,525.10,216.33,7.77;13,70.03,535.90,216.33,7.93;13,70.03,546.86,209.84,7.93" xml:id="b27">
	<analytic>
		<title level="a" type="main" coords="13,188.02,525.10,98.34,7.77;13,70.03,536.06,63.52,7.77">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="13,149.46,535.90,136.91,7.73;13,70.03,546.86,108.84,7.73">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,70.04,559.16,216.32,7.77;13,70.03,569.95,216.33,7.93;13,70.03,580.91,216.33,7.73;13,70.03,591.87,131.81,7.93" xml:id="b28">
	<analytic>
		<title level="a" type="main" coords="13,70.03,570.12,163.88,7.77">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="13,253.90,569.95,32.46,7.73;13,70.03,580.91,216.33,7.73;13,70.03,591.87,39.57,7.73">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,70.04,604.17,216.32,7.77;13,70.03,614.97,216.33,7.93;13,70.03,625.93,216.33,7.73;13,70.03,636.89,143.10,7.93" xml:id="b29">
	<analytic>
		<title level="a" type="main" coords="13,92.99,615.13,140.18,7.77">Relation networks for object detection</title>
		<author>
			<persName coords=""><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="13,253.90,614.97,32.46,7.73;13,70.03,625.93,216.33,7.73;13,70.03,636.89,41.70,7.73">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,70.04,649.19,216.32,7.77;13,70.03,659.99,216.33,7.93;13,70.03,670.94,216.33,7.73;13,70.03,681.90,157.38,7.93" xml:id="b30">
	<analytic>
		<title level="a" type="main" coords="13,265.94,649.19,20.42,7.77;13,70.03,660.15,143.24,7.77">Local relation networks for image recognition</title>
		<author>
			<persName coords=""><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="13,232.16,659.99,54.20,7.73;13,70.03,670.94,216.33,7.73;13,70.03,681.90,24.55,7.73">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2003">October 2019. 3</date>
			<biblScope unit="page" from="3464" to="3473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,70.04,694.20,216.32,7.77;13,70.03,705.16,216.33,7.77;13,328.78,75.96,216.33,7.94;13,328.78,87.08,76.34,7.77" xml:id="b31">
	<analytic>
		<title level="a" type="main" coords="13,139.69,705.16,131.46,7.77">Deep networks with stochastic depth</title>
		<author>
			<persName coords=""><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="13,328.78,75.96,151.41,7.73">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,328.79,99.80,216.32,7.77;13,328.78,110.75,216.33,7.77;13,328.78,121.71,57.28,7.77" xml:id="b32">
	<monogr>
		<title level="m" type="main" coords="13,328.78,110.75,216.33,7.77;13,328.78,121.71,23.76,7.77">Fapn: Feature-aligned pyramid network for dense image prediction</title>
		<author>
			<persName coords=""><forename type="first">Shihua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ran</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cheng</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,328.79,134.43,216.32,7.77;13,328.78,145.38,216.33,7.77;13,328.78,156.34,150.82,7.77" xml:id="b33">
	<monogr>
		<title level="m" type="main" coords="13,412.06,145.38,133.05,7.77;13,328.78,156.34,117.05,7.77">Shuffle transformer: Rethinking spatial shuffle for vision transformer</title>
		<author>
			<persName coords=""><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Youcheng</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guozhong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bin</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,328.79,169.06,216.32,7.77;13,328.78,180.01,216.33,7.77;13,328.78,190.97,74.73,7.77" xml:id="b34">
	<monogr>
		<title level="m" type="main" coords="13,468.54,169.06,76.57,7.77;13,328.78,180.01,216.33,7.77;13,328.78,190.97,41.57,7.77">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName coords=""><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,328.79,203.69,216.32,7.77;13,328.78,214.64,216.33,7.77;13,328.78,225.60,216.33,7.77;13,328.78,236.56,117.53,7.77" xml:id="b35">
	<monogr>
		<title level="m" type="main" coords="13,485.42,225.60,59.70,7.77;13,328.78,236.56,83.38,7.77">Scaling laws for neural language models</title>
		<author>
			<persName coords=""><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,328.79,249.27,216.32,7.77;13,328.78,260.23,216.33,7.77;13,328.78,271.19,216.33,7.77;13,328.78,281.99,216.33,7.93;13,328.78,293.11,27.89,7.77" xml:id="b36">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m" coords="13,487.09,271.19,58.02,7.77;13,328.78,282.15,88.03,7.77">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,328.79,305.82,216.32,7.77;13,328.78,316.78,153.64,7.77" xml:id="b37">
	<monogr>
		<title level="m" type="main" coords="13,467.00,305.82,78.11,7.77;13,328.78,316.78,120.08,7.77">Rethinking positional encoding in language pre-training</title>
		<author>
			<persName coords=""><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,328.79,329.49,216.32,7.77;13,328.78,340.45,216.33,7.77;13,328.78,351.41,216.33,7.77;13,328.78,362.21,186.70,7.93" xml:id="b38">
	<analytic>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11370</idno>
	</analytic>
	<monogr>
		<title level="m" coords="13,401.09,351.41,140.51,7.77">General visual representation learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,328.79,375.08,216.32,7.77;13,328.78,386.04,216.33,7.77;13,328.78,396.84,216.33,7.93;13,328.78,407.80,116.29,7.93" xml:id="b39">
	<analytic>
		<title level="a" type="main" coords="13,328.78,386.04,216.33,7.77;13,328.78,397.00,20.05,7.77">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="13,369.79,396.84,175.32,7.73;13,328.78,407.80,14.94,7.73">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,328.79,420.67,216.33,7.77;13,328.78,431.63,216.33,7.77;13,328.78,442.43,216.33,7.93;13,328.78,453.55,4.48,7.77" xml:id="b40">
	<analytic>
		<title level="a" type="main" coords="13,353.39,431.63,191.72,7.77;13,328.78,442.59,19.86,7.77">Gradient-based learning applied to document recognition</title>
		<author>
			<persName coords=""><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="13,357.28,442.43,88.38,7.73">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,328.79,466.26,216.32,7.77;13,328.78,477.22,216.33,7.77;13,328.78,488.18,60.25,7.77" xml:id="b41">
	<monogr>
		<title level="m" type="main" coords="13,388.43,477.22,156.69,7.77;13,328.78,488.18,26.36,7.77">Localvit: Bringing locality to vision transformers</title>
		<author>
			<persName coords=""><forename type="first">Yawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,328.79,500.89,216.32,7.77;13,328.78,511.85,216.33,7.77;13,328.78,522.81,216.33,7.77;13,328.78,533.77,80.43,7.77" xml:id="b42">
	<monogr>
		<title level="m" type="main" coords="13,531.66,511.85,13.45,7.77;13,328.78,522.81,216.33,7.77;13,328.78,533.77,46.79,7.77">Cbnetv2: A composite backbone network architecture for object detection</title>
		<author>
			<persName coords=""><forename type="first">Tingting</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaojie</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yudong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jingdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,328.79,546.48,216.32,7.77;13,328.78,557.44,216.33,7.77;13,328.78,568.40,216.33,7.77;13,328.78,579.19,216.33,7.93;13,328.78,590.31,62.89,7.77" xml:id="b43">
	<analytic>
		<title level="a" type="main" coords="13,363.63,568.40,163.74,7.77">Microsoft coco: Common objects in context</title>
		<author>
			<persName coords=""><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zitnick</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="13,328.78,579.19,151.41,7.73">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,328.79,603.03,216.32,7.77;13,328.78,613.98,216.33,7.77;13,328.78,624.94,46.32,7.77" xml:id="b44">
	<monogr>
		<title level="m" type="main" coords="13,538.64,603.03,6.47,7.77;13,328.78,613.98,216.33,7.77;13,328.78,624.94,12.95,7.77">A closer look at local aggregation operators in point cloud analysis</title>
		<author>
			<persName coords=""><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,328.79,637.66,216.32,7.77;13,328.78,648.61,216.33,7.77;13,328.78,659.57,216.33,7.77;13,328.78,670.53,100.39,7.77" xml:id="b45">
	<monogr>
		<title level="m" type="main" coords="13,503.29,648.61,41.83,7.77;13,328.78,659.57,216.33,7.77;13,328.78,670.53,16.76,7.77">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName coords=""><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2021. 2, 3, 4, 7, 9</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,328.79,683.24,216.32,7.77;13,328.78,694.20,216.33,7.77;13,328.78,705.16,13.45,7.77" xml:id="b46">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="13,432.99,694.20,84.84,7.77">Video swin transformer</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,70.04,76.13,216.32,7.77;14,70.03,86.92,216.33,7.94;14,70.03,97.88,103.26,7.94" xml:id="b47">
	<analytic>
		<title level="a" type="main" coords="14,204.51,76.13,81.85,7.77;14,70.03,87.08,63.12,7.77">Decoupled weight decay regularization</title>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="14,149.51,86.92,136.85,7.73;14,70.03,97.88,55.96,7.73">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,70.04,109.31,216.32,7.77;14,70.03,120.27,111.57,7.77" xml:id="b48">
	<monogr>
		<title level="m" type="main" coords="14,117.20,109.31,169.16,7.77;14,70.03,120.27,68.81,7.77">Turing-nlg: A 17-billion-parameter language model by microsoft</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Microsoft</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,70.04,131.55,216.32,7.77;14,70.03,142.50,216.33,7.77;14,70.03,153.46,124.64,7.77" xml:id="b49">
	<monogr>
		<title level="m" type="main" coords="14,109.95,131.55,176.41,7.77;14,70.03,142.50,216.33,7.77;14,70.03,153.46,81.44,7.77">Using deepspeed and megatron to train megatronturing nlg 530b, the world&apos;s largest and most powerful generative language model</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Microsoft</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,70.04,164.73,216.32,7.77;14,70.03,175.69,216.33,7.77;14,70.03,186.65,216.33,7.77;14,70.03,197.61,216.33,7.77;14,70.03,208.57,183.28,7.77" xml:id="b50">
	<monogr>
		<title level="m" type="main" coords="14,183.14,197.61,103.22,7.77;14,70.03,208.57,149.52,7.77">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName coords=""><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,70.04,219.84,216.32,7.77;14,70.03,230.80,216.33,7.77;14,70.03,241.76,136.46,7.77" xml:id="b51">
	<monogr>
		<title level="m" type="main" coords="14,175.74,230.80,110.63,7.77;14,70.03,241.76,84.01,7.77">Language models are unsupervised multitask learners</title>
		<author>
			<persName coords=""><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 1, 2, 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,70.04,253.03,216.32,7.77;14,70.03,263.99,216.33,7.77;14,70.03,274.95,216.33,7.77;14,70.03,285.75,216.34,7.93;14,70.03,296.71,150.20,7.93" xml:id="b52">
	<analytic>
		<title level="a" type="main" coords="14,117.73,274.95,168.63,7.77;14,70.03,285.91,109.78,7.77">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName coords=""><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="14,188.05,285.75,98.32,7.73;14,70.03,296.71,44.94,7.73">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020. 1, 2, 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,70.04,308.14,216.32,7.77;14,70.03,319.10,216.33,7.77;14,70.03,330.06,131.99,7.77" xml:id="b53">
	<monogr>
		<title level="m" type="main" coords="14,121.63,319.10,164.73,7.77;14,70.03,330.06,88.87,7.77">Zero: Memory optimizations toward training trillion parameter models</title>
		<author>
			<persName coords=""><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,70.04,341.33,216.32,7.77;14,70.03,352.29,216.33,7.77;14,70.03,363.25,68.72,7.77" xml:id="b54">
	<monogr>
		<title level="m" type="main" coords="14,138.24,352.29,148.13,7.77;14,70.03,363.25,25.92,7.77">Do imagenet classifiers generalize to imagenet?</title>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2019. 2, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,70.04,374.52,216.32,7.77;14,70.03,385.48,216.33,7.77;14,70.03,396.44,216.33,7.77;14,70.03,407.40,119.89,7.77" xml:id="b55">
	<monogr>
		<title level="m" type="main" coords="14,172.48,396.44,113.88,7.77;14,70.03,407.40,50.40,7.77">Scaling vision with sparse mixture of experts</title>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">André Susano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2021. 1, 2, 3, 5, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,70.04,418.67,216.32,7.77;14,70.03,429.63,216.33,7.77;14,70.03,440.58,189.66,7.77" xml:id="b56">
	<monogr>
		<title level="m" type="main" coords="14,192.28,429.63,94.09,7.77;14,70.03,440.58,146.86,7.77">Tokenlearner: What can 8 learned tokens do for images and videos?</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anurag</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mostafa</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anelia</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Angelova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,70.04,451.86,216.33,7.77;14,70.03,462.82,216.33,7.77;14,70.03,473.77,216.33,7.77;14,70.03,484.57,216.33,7.93;14,70.03,495.53,97.87,7.93" xml:id="b57">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Pieter-Jan</forename><surname>Kristof T Schütt</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Huziel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Sauceda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandre</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus-Robert</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Müller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08566</idno>
		<title level="m" coords="14,104.73,473.77,181.63,7.77;14,70.03,484.73,156.52,7.77">Schnet: A continuous-filter convolutional neural network for modeling quantum interactions</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,70.04,506.96,216.32,7.77;14,70.03,517.92,216.33,7.77;14,70.03,528.88,216.33,7.77;14,70.03,539.68,216.33,7.73;14,70.03,550.64,152.49,7.93" xml:id="b58">
	<analytic>
		<title level="a" type="main" coords="14,243.03,517.92,43.33,7.77;14,70.03,528.88,199.22,7.77">Objects365: A large-scale, high-quality dataset for object detection</title>
		<author>
			<persName coords=""><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="14,70.03,539.68,216.33,7.73;14,70.03,550.64,87.15,7.73">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2006">October 2019. 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,70.04,562.07,216.32,7.77;14,70.03,572.87,216.33,7.93;14,70.03,583.83,207.03,7.93" xml:id="b59">
	<analytic>
		<title level="a" type="main" coords="14,196.89,562.07,89.47,7.77;14,70.03,573.03,152.38,7.77">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="14,239.04,572.87,47.32,7.73;14,70.03,583.83,145.53,7.73">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015-05">May 2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,70.04,595.26,216.32,7.77;14,70.03,606.22,216.33,7.77;14,70.03,617.18,216.33,7.77;14,70.03,627.98,216.33,7.93;14,70.03,638.94,97.87,7.93" xml:id="b60">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12450</idno>
		<title level="m" coords="14,189.56,617.18,96.80,7.77;14,70.03,628.14,151.71,7.77">Sparse r-cnn: End-to-end object detection with learnable proposals</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,70.04,650.37,216.32,7.77;14,70.03,661.33,216.33,7.77;14,70.03,672.29,216.33,7.77;14,70.03,683.08,216.33,7.93;14,70.03,694.04,216.33,7.93;14,70.03,705.16,4.48,7.77" xml:id="b61">
	<analytic>
		<title level="a" type="main" coords="14,216.64,672.29,69.72,7.77;14,70.03,683.24,44.34,7.77">Going deeper with convolutions</title>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="14,137.58,683.08,148.78,7.73;14,70.03,694.04,146.67,7.73">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,328.79,76.13,216.32,7.77;14,328.78,87.08,216.33,7.77;14,328.78,98.04,216.33,7.77;14,328.78,108.84,181.71,7.93" xml:id="b62">
	<monogr>
		<title level="m" type="main" coords="14,514.55,87.08,30.57,7.77;14,328.78,98.04,216.33,7.77;14,328.78,109.00,23.76,7.77">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,328.79,120.27,216.32,7.77;14,328.78,131.23,216.33,7.77;14,328.78,142.19,46.32,7.77" xml:id="b63">
	<monogr>
		<title level="m" type="main" coords="14,534.65,120.27,10.45,7.77;14,328.78,131.23,216.33,7.77;14,328.78,142.19,12.95,7.77">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName coords=""><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,328.79,153.46,216.32,7.77;14,328.78,164.42,216.33,7.77;14,328.78,175.22,216.33,7.93;14,328.78,186.18,216.33,7.93;14,328.78,197.30,13.45,7.77" xml:id="b64">
	<analytic>
		<title level="a" type="main" coords="14,373.95,175.38,85.36,7.77">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="14,475.13,175.22,69.98,7.73;14,328.78,186.18,113.29,7.73">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2017. 2, 3, 8</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,328.79,208.57,216.32,7.77;14,328.78,219.53,216.33,7.77;14,328.78,230.49,68.49,7.77" xml:id="b65">
	<monogr>
		<title level="m" type="main" coords="14,328.78,219.53,216.33,7.77;14,328.78,230.49,35.00,7.77">You only learn one representation: Unified network for multiple tasks</title>
		<author>
			<persName coords=""><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I-Hau</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,328.79,241.76,216.32,7.77;14,328.78,252.72,216.33,7.77;14,328.78,263.52,216.33,7.93;14,328.78,274.48,216.33,7.93;14,328.78,285.60,4.48,7.77" xml:id="b66">
	<analytic>
		<title level="a" type="main" coords="14,452.06,252.72,93.05,7.77;14,328.78,263.68,120.97,7.77">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simon</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrei</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="14,476.48,263.52,68.64,7.73;14,328.78,274.48,176.09,7.73">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003">2018. Jun 2018. 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,328.79,296.87,216.32,7.77;14,328.78,307.83,216.33,7.77;14,328.78,318.79,216.33,7.77;14,328.78,329.74,134.95,7.77" xml:id="b67">
	<monogr>
		<title level="m" type="main" coords="14,525.69,307.83,19.42,7.77;14,328.78,318.79,216.33,7.77;14,328.78,329.74,101.13,7.77">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName coords=""><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,328.79,341.02,216.32,7.77;14,328.78,351.97,216.33,7.77;14,328.78,362.93,145.81,7.77" xml:id="b68">
	<monogr>
		<title level="m" type="main" coords="14,391.85,351.97,153.26,7.77;14,328.78,362.93,112.04,7.77">Rethinking and improving relative position encoding for vision transformer</title>
		<author>
			<persName coords=""><forename type="first">Kan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,328.79,374.04,216.33,7.93;14,328.78,385.00,216.33,7.73;14,328.78,395.96,106.33,7.93" xml:id="b69">
	<analytic>
		<title level="a" type="main" coords="14,436.00,374.21,74.05,7.77">Group normalization</title>
		<author>
			<persName coords=""><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="14,529.08,374.04,16.03,7.73;14,328.78,385.00,216.33,7.73;14,328.78,395.96,26.68,7.73">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,328.79,407.39,216.32,7.77;14,328.78,418.35,216.33,7.77;14,328.78,429.15,216.33,7.93;14,328.78,440.27,4.48,7.77" xml:id="b70">
	<monogr>
		<title level="m" type="main" coords="14,434.06,418.35,111.06,7.77;14,328.78,429.31,63.64,7.77">Early convolutions help transformers see better</title>
		<author>
			<persName coords=""><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.14881</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,328.79,451.54,216.32,7.77;14,328.78,462.50,216.33,7.77;14,328.78,473.30,216.33,7.93;14,328.78,484.42,45.82,7.77" xml:id="b71">
	<monogr>
		<title level="m" type="main" coords="14,482.99,462.50,62.12,7.77;14,328.78,473.46,154.62,7.77">Simmim: A simple framework for masked image modeling</title>
		<author>
			<persName coords=""><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Tech report</note>
</biblStruct>

<biblStruct coords="14,328.79,495.69,216.32,7.77;14,328.78,506.65,216.33,7.77;14,328.78,517.61,216.33,7.77;14,328.78,528.57,102.32,7.77" xml:id="b72">
	<monogr>
		<title level="m" type="main" coords="14,417.42,517.61,127.69,7.77;14,328.78,528.57,67.77,7.77">On layer normalization in the transformer architecture</title>
		<author>
			<persName coords=""><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,328.79,539.84,216.32,7.77;14,328.78,550.80,216.33,7.77;14,328.78,561.76,216.33,7.77;14,328.78,572.72,13.45,7.77" xml:id="b73">
	<monogr>
		<title level="m" type="main" coords="14,517.72,550.80,27.39,7.77;14,328.78,561.76,190.76,7.77">End-toend semi-supervised object detection with soft teacher</title>
		<author>
			<persName coords=""><forename type="first">Mengde</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,328.79,583.99,216.32,7.77;14,328.78,594.95,216.33,7.77;14,328.78,605.91,216.33,7.77;14,328.78,616.87,4.48,7.77" xml:id="b74">
	<monogr>
		<title level="m" type="main" coords="14,475.16,594.95,69.96,7.77;14,328.78,605.91,179.75,7.77">Focal self-attention for local-global interactions in vision transformers</title>
		<author>
			<persName coords=""><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,328.79,628.14,216.32,7.77;14,328.78,639.10,216.33,7.77;14,328.78,649.89,216.33,7.93;14,328.78,660.85,205.71,7.93" xml:id="b75">
	<analytic>
		<title level="a" type="main" coords="14,386.89,639.10,158.22,7.77;14,328.78,650.06,43.90,7.77">Leveraging batch normalization for vision transformers</title>
		<author>
			<persName coords=""><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="14,388.54,649.89,156.57,7.73;14,328.78,660.85,113.56,7.73">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="413" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,328.79,672.29,216.32,7.77;14,328.78,683.24,216.33,7.77;14,328.78,694.20,216.33,7.77;14,328.78,705.16,104.08,7.77" xml:id="b76">
	<monogr>
		<title level="m" type="main" coords="14,348.13,694.20,196.98,7.77;14,328.78,705.16,70.05,7.77">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Francis</forename><forename type="middle">Eh</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,70.04,76.13,216.32,7.77;15,70.03,87.08,216.33,7.77;15,70.03,97.88,177.73,7.94" xml:id="b77">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13112</idno>
		<title level="m" coords="15,159.76,87.08,126.61,7.77;15,70.03,98.04,19.86,7.77">Vision outlooker for visual recognition</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,70.04,110.00,216.32,7.77;15,70.03,120.96,216.33,7.77;15,70.03,131.92,216.33,7.77;15,70.03,142.71,216.33,7.93;15,70.03,153.67,199.73,7.93" xml:id="b78">
	<analytic>
		<title level="a" type="main" coords="15,222.76,120.96,63.60,7.77;15,70.03,131.92,216.33,7.77;15,70.03,142.87,16.39,7.77">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName coords=""><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="15,105.41,142.71,180.96,7.73;15,70.03,153.67,98.61,7.73">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,70.04,165.79,216.32,7.77;15,70.03,176.75,216.33,7.77;15,70.03,187.71,4.48,7.77" xml:id="b79">
	<monogr>
		<title level="m" type="main" coords="15,114.72,176.75,99.46,7.77">Scaling vision transformers</title>
		<author>
			<persName coords=""><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2021. 1, 2, 3, 5, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,70.04,199.66,216.32,7.77;15,70.03,210.62,216.33,7.77;15,70.03,221.42,170.75,7.93" xml:id="b80">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m" coords="15,138.52,210.62,147.85,7.77;15,70.03,221.58,12.95,7.77">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,70.04,233.53,216.32,7.77;15,70.03,244.49,216.33,7.77;15,70.03,255.45,216.33,7.77;15,70.03,266.41,65.24,7.77" xml:id="b81">
	<monogr>
		<title level="m" type="main" coords="15,200.77,244.49,85.59,7.77;15,70.03,255.45,216.33,7.77;15,70.03,266.41,31.21,7.77">Multi-scale vision longformer: A new vision transformer for high-resolution image encoding</title>
		<author>
			<persName coords=""><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,70.04,278.37,216.32,7.77;15,70.03,289.33,216.33,7.77;15,70.03,300.28,216.33,7.77;15,70.03,311.24,216.33,7.77;15,70.03,322.04,184.69,7.93" xml:id="b82">
	<monogr>
		<title level="m" type="main" coords="15,178.77,300.28,107.60,7.77;15,70.03,311.24,216.33,7.77;15,70.03,322.20,26.36,7.77">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName coords=""><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Philip Hs Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15840</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,70.04,334.16,216.32,7.77;15,70.03,344.96,216.33,7.93;15,70.03,355.91,216.33,7.93;15,70.03,367.03,117.79,7.77" xml:id="b83">
	<analytic>
		<title level="a" type="main" coords="15,103.13,345.12,123.55,7.77">Random erasing data augmentation</title>
		<author>
			<persName coords=""><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="15,241.94,344.96,44.42,7.73;15,70.03,355.91,171.24,7.73">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,70.04,378.99,216.32,7.77;15,70.03,389.95,216.33,7.77;15,70.03,400.75,216.33,7.93;15,70.03,411.71,133.85,7.93" xml:id="b84">
	<analytic>
		<title level="a" type="main" coords="15,227.79,389.95,58.58,7.77;15,70.03,400.91,162.60,7.77">Semantic understanding of scenes through the ade20k dataset</title>
		<author>
			<persName coords=""><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="15,239.04,400.75,47.32,7.73;15,70.03,411.71,100.22,7.73">International Journal on Computer Vision</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
